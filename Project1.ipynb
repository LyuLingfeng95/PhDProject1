{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704c30d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import difflib\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "from random import sample\n",
    "\n",
    "import folium\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "import seaborn as sns\n",
    "import statsmodels.tools.tools as sm\n",
    "import statsmodels.base as sb\n",
    "from folium.features import CustomIcon\n",
    "from folium.plugins import FastMarkerCluster, HeatMap, MarkerCluster\n",
    "from linearmodels.panel.model import PooledOLS\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.tools.eval_measures import aic, rmse\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import yfinance as yf\n",
    "from yahoofinancials import YahooFinancials\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b101ee7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2668\n",
      "2669\n"
     ]
    }
   ],
   "source": [
    "postcodecsv = pd.read_csv(\"au_postcodes.csv\")\n",
    "# References:\n",
    "# https://blog.greglow.com/2019/11/05/power-bi-creating-a-topojson-file-of-australian-postcodes-for-use-with-shape-map/\n",
    "# https://mapshaper.org/\n",
    "# https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003July%202016?OpenDocument\n",
    "\n",
    "from statistics import mean, median\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "ndf = pd.read_json(\"POA_2016_AUST.json\")\n",
    "ndf1 = ndf[\"features\"].apply(pd.Series)\n",
    "ndf2 = ndf1[\"geometry\"].apply(pd.Series)\n",
    "ndf3 = ndf1[\"properties\"].apply(pd.Series)\n",
    "border = ndf2[\"coordinates\"]\n",
    "\n",
    "depth = lambda L: isinstance(L, list) and max(map(depth, L)) + 1\n",
    "\n",
    "\n",
    "def flatten(d, l=1):\n",
    "    for i in d:\n",
    "        yield from ([i] if l == 1 else flatten(i, l - 1))\n",
    "\n",
    "\n",
    "def centerlocation(index):\n",
    "    tmplst = list(flatten(border[index], l=depth(border[index])))\n",
    "    longi = median([i for i in tmplst if i > 0])\n",
    "    lati = median([i for i in tmplst if i < 0])\n",
    "    return [longi, lati]\n",
    "\n",
    "\n",
    "postcodedf = []\n",
    "i = 0\n",
    "for i in range(len(ndf)):\n",
    "    if depth(border[i]) > 0:\n",
    "        tpd = {\n",
    "            \"postcode\": [ndf3[\"POA_CODE16\"].iloc[i]],\n",
    "            \"longitude\": [centerlocation(i)[0]],\n",
    "            \"latitude\": [centerlocation(i)[1]],\n",
    "        }\n",
    "        tpdf = pd.DataFrame(tpd)\n",
    "        postcodedf.append(tpdf)\n",
    "    else:\n",
    "        print(i)\n",
    "        continue\n",
    "    i = i + 1\n",
    "postcodedf = pd.concat(postcodedf, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2906c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b119f30a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"./markettrends0.csv\",\n",
    "    dtype={\n",
    "        \"state\": \"str\",\n",
    "        \"sa3_name16\": \"str\",\n",
    "        \"sa4_name16\": \"str\",\n",
    "        \"postcode\": \"str\",\n",
    "        \"state\": \"str\",\n",
    "        \"property_type\": \"str\",\n",
    "    },\n",
    ")\n",
    "ndata = data.fillna(\n",
    "    {\"Volume of new rental listings (1 month)\": 0, \"Volume of sales (1 month)\": 0}\n",
    ").dropna(subset=[\"postcode\"])\n",
    "ndatahouses = ndata[:][ndata.property_type == \"Houses\"]\n",
    "ndataunits = ndata[:][ndata.property_type == \"Units\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75e8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndatahouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93cd585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndatahouses['postcode'].loc[ndatahouses['sa3_name16'] == 'Darwin City' ].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19af3095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# So far, Houses only\n",
    "UniqueNames_sa3 = ndatahouses.sa3_name16.unique()[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc4b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameDict_sa3 \n",
    "# DataFrameDict_sa3 = {elem: pd.DataFrame for elem in UniqueNames_sa3}\n",
    "# for key in DataFrameDict_sa3.keys():\n",
    "#     DataFrameDict_sa3[key] = ndata[:][ndata.sa3_name16 == key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae104ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = list(range(2001, 2021))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75b5df9-2d4b-4b0e-897e-608860ea0477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrameDict_HILDAp = {elem: pd.DataFrame for elem in year}\n",
    "for key in DataFrameDict_HILDAp.keys():\n",
    "    order = key - 2001\n",
    "    ini = string.ascii_lowercase[order]\n",
    "    df, meta = pyreadstat.read_dta(f\"Rperson_{ini}200u.dta\")\n",
    "    df.columns = df.columns.str[1:]\n",
    "    DataFrameDict_HILDAp[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c834559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle1 \n",
    "# with open('DataFrameDict_HILDAp.pickle', 'wb') as f:\n",
    "#     pickle.dump(DataFrameDict_HILDAp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea5de63-1c94-4399-a2a9-6edee0d5ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrameDict_HILDA_cw = {elem: pd.DataFrame for elem in year}\n",
    "for key in DataFrameDict_HILDA_cw.keys():\n",
    "    order = key - 2001\n",
    "    ini = string.ascii_lowercase[order]\n",
    "    df, meta = pyreadstat.read_dta(f\"Combined_{ini}200u.dta\")\n",
    "    df.columns = df.columns.str[1:]\n",
    "    DataFrameDict_HILDA_cw[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3783dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DataFrameDict_HILDA_cw.pickle', 'wb') as f:\n",
    "    pickle.dump(DataFrameDict_HILDA_cw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3fb64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrameDict_HILDA = {elem: pd.DataFrame for elem in year}\n",
    "for key in DataFrameDict_HILDA.keys():\n",
    "    order = key - 2001\n",
    "    ini = string.ascii_lowercase[order]\n",
    "    df, meta = pyreadstat.read_dta(f\"Household_{ini}200u.dta\")\n",
    "    df.columns = df.columns.str[1:]\n",
    "    DataFrameDict_HILDA[key] = df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ba056",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DataFrameDict_HILDA.pickle', 'wb') as f:\n",
    "    pickle.dump(DataFrameDict_HILDA, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_postcodes = DataFrameDict_HILDA[2001][\"hhpcode\"]\n",
    "for key in DataFrameDict_HILDA.keys():\n",
    "    list_ny = DataFrameDict_HILDA[key][\"hhpcode\"]\n",
    "    list_postcodes = list(set(list_postcodes).intersection(list_ny))\n",
    "count_dict = dict()\n",
    "for key in DataFrameDict_HILDA.keys():\n",
    "    for item in DataFrameDict_HILDA[key][\"hhpcode\"].unique():\n",
    "        if item in count_dict:\n",
    "            count_dict[item] += 1\n",
    "        else:\n",
    "            count_dict[item] = 1\n",
    "df_allpostcode = pd.DataFrame(count_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HILDAallpc = df_allpostcode[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3681d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_retainedpc = df_allpostcode.drop(df_allpostcode[df_allpostcode[1] < 20].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c78316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "UniquePostcode = df_retainedpc[0]\n",
    "DataFrameDict_postcode = {elem: pd.DataFrame for elem in UniquePostcode}\n",
    "for key in DataFrameDict_postcode.keys():\n",
    "    DataFrameDict_postcode[key] = ndatahouses[:][ndatahouses.postcode == key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcde3c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unique_non_null(s):\n",
    "    return s.dropna().unique()\n",
    "\n",
    "\n",
    "# placepc  = postcodedf['Postcode']\n",
    "\n",
    "growthrate = []\n",
    "\n",
    "for key in DataFrameDict_postcode.keys():\n",
    "    if sum(postcodedf[\"postcode\"] == key) > 0:\n",
    "        # ind = placepc.loc[placepc == pc].index[0]\n",
    "        ind = postcodedf.loc[postcodedf[\"postcode\"] == key].index[0]\n",
    "        df = DataFrameDict_postcode[key]\n",
    "        if len(df[\"Hedonic Home Value Index\"]) > 360:\n",
    "            tpd = {\n",
    "                \"UniqueNames_sa3\": DataFrameDict_postcode[key][\"sa3_name16\"].unique(),\n",
    "                \"S/T\": DataFrameDict_postcode[key][\"state\"].unique(),\n",
    "                #'Postcode':[postcodedf.loc[ind,'Postcode']],\n",
    "                # 'longitude':[postcodedf.loc[ind,'longitude']],\n",
    "                #'latitude':[postcodedf.loc[ind,'latitude']],\n",
    "                \"Postcode\": [postcodedf.loc[ind, \"postcode\"]],\n",
    "                \"longitude\": [postcodedf.loc[ind, \"longitude\"]],\n",
    "                \"latitude\": [postcodedf.loc[ind, \"latitude\"]],\n",
    "                \"Growth_Houses\": [\n",
    "                    df[\"Hedonic Home Value Index\"].iloc[-1]\n",
    "                    / df[\"Hedonic Home Value Index\"].iloc[359]\n",
    "                ],\n",
    "                \"FinalHPI_Houses\": [df[\"Hedonic Home Value Index\"].iloc[-1]],\n",
    "                # 'Suburbs':\", \". join( ((postcodecsv.loc[(postcodecsv['postcode'] ==int(postcodedf.loc[ind,'Postcode']))]['place_name']).tolist()))}\n",
    "                \"Suburbs\": \", \".join(\n",
    "                    (\n",
    "                        (\n",
    "                            postcodecsv.loc[(postcodecsv[\"postcode\"] == int(key))][\n",
    "                                \"place_name\"\n",
    "                            ]\n",
    "                        ).tolist()\n",
    "                    )\n",
    "                ),\n",
    "            }\n",
    "            tpdf = pd.DataFrame(tpd)\n",
    "            growthrate.append(tpdf)\n",
    "    else:\n",
    "        print(\"area(value) %s: data incomplete\" % (key))\n",
    "        continue\n",
    "\n",
    "growthrate = pd.concat(growthrate, ignore_index=True)\n",
    "growthrate[\"Postcode\"] = growthrate[\"Postcode\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0225f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfsa3 = pd.read_json(\"SA3_2016_AUST.json\")\n",
    "ndf1sa3 = ndfsa3[\"features\"].apply(pd.Series)\n",
    "ndf2sa3 = ndf1sa3[\"geometry\"].apply(pd.Series)\n",
    "ndf3sa3 = ndf1sa3[\"properties\"].apply(pd.Series)\n",
    "borderdsa3 = ndf2sa3[\"coordinates\"]\n",
    "placename = ndf3sa3[\"SA3_NAME16\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f022d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "growthsa3 = []\n",
    "\n",
    "i = -1\n",
    "\n",
    "for key in DataFrameDict_sa3.keys():\n",
    "    i = i + 1\n",
    "    ind = placename[placename == difflib.get_close_matches(key, placename)[0]].index[0]\n",
    "    df = DataFrameDict_sa3[key]\n",
    "    tpd = {\n",
    "        \"UniqueNames_sa3\": [DataFrameDict_sa3[key][\"sa3_name16\"].unique()[0]],\n",
    "        \"sa3code\": [ndf3sa3.loc[ind, \"SA3_CODE16\"]],\n",
    "        \"FinalHPI_Houses\": [\n",
    "            DataFrameDict_sa3[key][df.value_at_date == \"2021-05-31\"][\n",
    "                \"Hedonic Home Value Index\"\n",
    "            ].mean()\n",
    "        ],\n",
    "    }\n",
    "    tpdf = pd.DataFrame(tpd)\n",
    "    growthsa3.append(tpdf)\n",
    "growthsa3 = pd.concat(growthsa3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67b3dc-a2e7-4bb1-abc2-71809f645062",
   "metadata": {},
   "source": [
    "# New Map Updated\n",
    "\n",
    "- Suburbs without enough samples in HILDA are removed. \n",
    "- Standard of this version: missing rate is greater than 0% or suburbs not included in HILDA data \n",
    "- Link: https://lyulingfeng95.github.io/Project1_LF/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7cb807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folium_map4 = folium.Map(location=[-30, 138], zoom_start=4.4, titles=\"cartodbpositron\")\n",
    "\n",
    "\n",
    "f = open(\"POA_2016_AUST.json\")\n",
    "postcode_geo = json.load(f)\n",
    "postcodedata2 = pd.DataFrame(\n",
    "    {\"postcode\": df_retainedpc[0], \"fill\": [1] * len(df_retainedpc[0])}\n",
    ")\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=postcode_geo,\n",
    "    name=\"choropleth\",\n",
    "    data=postcodedata2,\n",
    "    fill_color=\"BrBG\",\n",
    "    columns=[\"postcode\", \"fill\"],\n",
    "    key_on=\"feature.properties.POA_CODE16\",\n",
    "    fill_opacity=0.4,\n",
    "    line_opacity=0.6,\n",
    ").add_to(folium_map4)\n",
    "\n",
    "mc1 = MarkerCluster(name=\"Marker Cluster\")\n",
    "df = growthrate\n",
    "for index, row in df.iterrows():\n",
    "    html = \" Postcode: {}<br><br>Suburb(s): {}<br><br> Location (SA3): {} <br><br>State: {} <br><br> Fixed Based Growth Rate since 31/12/2009: {}  \".format(\n",
    "        row[\"Postcode\"],\n",
    "        row[\"Suburbs\"],\n",
    "        row[\"UniqueNames_sa3\"],\n",
    "        row[\"S/T\"],\n",
    "        format(row[\"Growth_Houses\"], \".4f\"),\n",
    "    )\n",
    "\n",
    "    iframe = folium.IFrame(html)\n",
    "    popup = folium.Popup(iframe, min_width=400, max_width=400)\n",
    "    folium.CircleMarker(\n",
    "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "        color=\"blue\",\n",
    "        fill_opacity=math.pow(row[\"Growth_Houses\"], 4) / 10,\n",
    "        weight=0.8,\n",
    "        popup=popup,\n",
    "        fill=True,\n",
    "    ).add_to(mc1)\n",
    "\n",
    "mc1.add_to(folium_map4)\n",
    "\n",
    "folium.LayerControl().add_to(folium_map4)\n",
    "\n",
    "folium_map4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce7326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# folium_map4.save(\"index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4a483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The average house price index growth rate.\n",
    "aveHPI = []\n",
    "lst = [\"postcode\", \"date\", \"logHPI\", \"logHPIdiff\"]\n",
    "# Calling DataFrame constructor on list\n",
    "for key in DataFrameDict_postcode.keys():\n",
    "    df = pd.DataFrame([], columns=lst)\n",
    "    df[\"Description\"] = DataFrameDict_postcode[key][\"value_at_date\"].loc[\n",
    "        (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "        & (DataFrameDict_postcode[key][\"value_at_date\"] > \"1990-12\")\n",
    "    ]\n",
    "    df[\"logHPI\"] = np.log2(\n",
    "        DataFrameDict_postcode[key][\"Hedonic Home Value Index\"].loc[\n",
    "            (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "            & (DataFrameDict_postcode[key][\"value_at_date\"] > \"1990-12\")\n",
    "        ]\n",
    "    )\n",
    "    df[\"postcode\"] = key\n",
    "    df[\"logHPIdiff\"] = df[\"logHPI\"].diff(1)\n",
    "    aveHPI.append(df)\n",
    "\n",
    "aveHPIdf = pd.concat(aveHPI)\n",
    "aveHPIdf = aveHPIdf.reset_index(drop=True)\n",
    "agg = aveHPIdf.groupby(\"Description\")[\"logHPIdiff\"].agg([\"mean\"]).reset_index()\n",
    "agg.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b8805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xls1 = pd.read_excel(\"f01hist.xls\", sheet_name=\"Data\", header=2)\n",
    "xls1 = xls1.drop(xls1.index[0:8], axis=0)\n",
    "xls1[\"Description\"] = pd.to_datetime(xls1[\"Description\"])\n",
    "xls1[\"Description\"] = xls1[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "xls3 = pd.read_excel(\"f11hist-1969-2009.xls\", sheet_name=\"Data\", header=2)\n",
    "xls3 = xls3.drop(xls3.index[0:8], axis=0)\n",
    "xls3[\"Description\"] = pd.to_datetime(xls3[\"Description\"])\n",
    "xls3[\"Description\"] = xls3[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "xls4 = pd.read_excel(\"f11hist.xls\", sheet_name=\"Data\", header=2)\n",
    "xls4 = xls4.drop(xls4.index[0:8], axis=0)\n",
    "xls4[\"Description\"] = pd.to_datetime(xls4[\"Description\"])\n",
    "xls4[\"Description\"] = xls4[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "xls34 = pd.concat([xls3, xls4], axis=0)\n",
    "xls5 = pd.read_excel(\"g01hist.xls\", sheet_name=\"Data\", header=2)\n",
    "xls5 = xls5.drop(xls5.index[0:8], axis=0)\n",
    "xls5[\"Description\"] = pd.to_datetime(xls5[\"Description\"])\n",
    "xls5[\"Description\"] = xls5[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "xls6 = pd.read_excel(\"h01hist.xls\", sheet_name=\"Data\", header=2)\n",
    "xls6 = xls6.drop(xls6.index[0:8], axis=0)\n",
    "xls6[\"Description\"] = pd.to_datetime(xls6[\"Description\"])\n",
    "xls6[\"Description\"] = xls6[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "xls7 = pd.read_excel(\"h03hist.xls\", sheet_name=\"Data\", header=2)\n",
    "xls7 = xls7.drop(xls7.index[0:8], axis=0)\n",
    "xls7[\"Description\"] = pd.to_datetime(xls7[\"Description\"])\n",
    "xls7[\"Description\"] = xls7[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "# Download ASX data from Yahoo Finance\n",
    "\n",
    "ASX200data = yf.download('^AXJO', start='1999-01-01', end='2021-06-01', interval = \"1mo\", progress=False)\n",
    "ASX200data = ASX200data.reset_index()\n",
    "ASX200data[\"Description\"] = pd.to_datetime(ASX200data[\"Date\"])\n",
    "ASX200data[\"Description\"] = ASX200data[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69db52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# macvar = pd.DataFrame().assign(\n",
    "#     Description=xls7.loc[\n",
    "#         (\"2021-06\" > xls7[\"Description\"]) & (xls7[\"Description\"] > \"1998-12\")\n",
    "#     ][\"Description\"]\n",
    "# )\n",
    "# macvar.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# macvar = pd.merge(\n",
    "#     macvar,\n",
    "#     xls1[[\"Description\", \"Cash Rate Target; monthly average\"]],\n",
    "#     on=\"Description\",\n",
    "#     how=\"left\",\n",
    "# )\n",
    "# macvar = pd.merge(\n",
    "#     macvar,\n",
    "#     xls34[[\"Description\", \"AUD/USD Exchange Rate; see notes for further detail.\"]],\n",
    "#     on=\"Description\",\n",
    "#     how=\"left\",\n",
    "# )\n",
    "# macvar = pd.merge(\n",
    "#     macvar,\n",
    "#     xls5[[\"Description\", \"Consumer price index; All groups\"]],\n",
    "#     on=\"Description\",\n",
    "#     how=\"left\",\n",
    "# )\n",
    "# macvar = pd.merge(\n",
    "#     macvar,\n",
    "#     xls6[[\"Description\", \"Gross domestic product (GDP); Chain volume\"]],\n",
    "#     on=\"Description\",\n",
    "#     how=\"left\",\n",
    "# )\n",
    "# macvar = pd.merge(\n",
    "#     macvar,\n",
    "#     xls7[[\"Description\", \"Retail sales; All industries; Current price\"]],\n",
    "#     on=\"Description\",\n",
    "#     how=\"left\",\n",
    "# )\n",
    "# macvar = pd.merge(\n",
    "#     macvar,\n",
    "#     xls7[[\"Description\", \"Private dwelling approvals\"]],\n",
    "#     on=\"Description\",\n",
    "#     how=\"left\",\n",
    "# )\n",
    "\n",
    "# macvar = macvar.set_axis(\n",
    "#     [\"date\", \"ir\", \"exr\", \"cpi\", \"gdp\", \"rs\", \"pda\"], axis=1, inplace=False\n",
    "# )\n",
    "# macvar[\"ir\"] = macvar[\"ir\"].astype(float, errors=\"raise\")\n",
    "# macvar[\"exr\"] = macvar[\"exr\"].astype(float, errors=\"raise\")\n",
    "# macvar[\"cpi\"] = macvar[\"cpi\"].astype(float, errors=\"raise\")\n",
    "# macvar[\"gdp\"] = macvar[\"gdp\"].astype(float, errors=\"raise\")\n",
    "# macvar[\"rs\"] = macvar[\"rs\"].astype(float, errors=\"raise\")\n",
    "# macvar[\"pda\"] = macvar[\"pda\"].astype(float, errors=\"raise\")\n",
    "# macvar = macvar.interpolate()\n",
    "# macvarst = macvar\n",
    "\n",
    "\n",
    "# lag = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "# threshold = 0.01\n",
    "# for i in lag:\n",
    "#     if adfuller(macvar[\"ir\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"ir{i}\"] = macvar[\"ir\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"ir\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"ir{i}_2\"] = macvar[\"ir\"].pct_change(i).diff()\n",
    "\n",
    "#     if adfuller(macvar[\"exr\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"exr{i}\"] = macvar[\"exr\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"exr\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"exr{i}_2\"] = macvar[\"exr\"].pct_change(i).diff()\n",
    "\n",
    "#     if adfuller(macvar[\"cpi\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"cpi{i}\"] = macvar[\"cpi\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"cpi\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"cpi{i}_2\"] = macvar[\"cpi\"].pct_change(i).diff()\n",
    "\n",
    "#     if adfuller(macvar[\"gdp\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"gdp{i}\"] = macvar[\"gdp\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"gdp\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"gdp{i}_2\"] = macvar[\"gdp\"].pct_change(i).diff()\n",
    "\n",
    "#     if adfuller(macvar[\"rs\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"rs{i}\"] = macvar[\"rs\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"rs\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"rs{i}_2\"] = macvar[\"rs\"].pct_change(i).diff()\n",
    "\n",
    "#     if adfuller(macvar[\"pda\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"pda{i}\"] = macvar[\"pda\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"pda\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"pda{i}_2\"] = macvar[\"pda\"].pct_change(i).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "macvar = pd.DataFrame().assign(\n",
    "    Description=xls7.loc[\n",
    "        (\"2021-06\" > xls7[\"Description\"]) & (xls7[\"Description\"] > \"1998-12\")\n",
    "    ][\"Description\"]\n",
    ")\n",
    "macvar.reset_index(drop=True, inplace=True)\n",
    "\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls1[[\"Description\", \"Cash Rate Target; monthly average\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls34[[\"Description\", \"AUD/USD Exchange Rate; see notes for further detail.\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls5[[\"Description\", \"Consumer price index; All groups\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls6[[\"Description\", \"Gross domestic product (GDP); Chain volume\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls7[[\"Description\", \"Retail sales; All industries; Current price\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls7[[\"Description\", \"Private dwelling approvals\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    ASX200data[[\"Description\", \"Adj Close\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ddff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "macvar = macvar.set_axis(\n",
    "    [\"date\", \"ir\", \"exr\", \"cpi\", \"gdp\", \"rs\", \"pda\", \"asx\"], axis=1, inplace=False\n",
    ")\n",
    "\n",
    "\n",
    "macvar[\"ir\"] = macvar[\"ir\"].astype(float, errors=\"raise\")\n",
    "macvar[\"exr\"] = macvar[\"exr\"].astype(float, errors=\"raise\")\n",
    "macvar[\"cpi\"] = macvar[\"cpi\"].astype(float, errors=\"raise\")\n",
    "macvar[\"gdp\"] = macvar[\"gdp\"].astype(float, errors=\"raise\")\n",
    "macvar[\"rs\"] = macvar[\"rs\"].astype(float, errors=\"raise\")\n",
    "macvar[\"pda\"] = macvar[\"pda\"].astype(float, errors=\"raise\")\n",
    "macvar[\"asx\"] = macvar[\"asx\"].astype(float, errors=\"raise\")\n",
    "macvar = macvar.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445bc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "macvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function used to obtain a stationay series from the original series\n",
    "## Order: series itself; 1sr order; seasonal order; annual order; 2nd order. \n",
    "threshold = 0.01\n",
    "\n",
    "def selectStationaySeries(variable_tar):\n",
    "        if adfuller(variable_tar.dropna(how=\"all\"))[1] < threshold:\n",
    "            stationary_variable = variable_tar\n",
    "            suffix =  \"original\"\n",
    "        elif adfuller(variable_tar.pct_change(1).dropna(how=\"all\"))[1] < threshold:\n",
    "            stationary_variable = variable_tar.pct_change(1).dropna(how=\"all\")\n",
    "            suffix = \"1storderdiff\"\n",
    "        elif adfuller(variable_tar.pct_change(3).dropna(how=\"all\"))[1] < threshold:\n",
    "            stationary_variable = variable_tar.pct_change(3).dropna(how=\"all\")\n",
    "            suffix = \"seasonaldiff\"\n",
    "        elif adfuller(variable_tar.pct_change(12).dropna(how=\"all\"))[1] < threshold:\n",
    "            stationary_variable = variable_tar.pct_change(12).dropna(how=\"all\")\n",
    "            suffix = \"annualdiff\"     \n",
    "        elif adfuller(variable_tar.pct_change(1).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "            stationary_variable = variable_tar.pct_change(1).diff().dropna(how=\"all\")\n",
    "            suffix = \"2ndorderdiff\"\n",
    "        else:\n",
    "            print(\"not found\")\n",
    "        return(pd.DataFrame(stationary_variable))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adacecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "macvarst = macvar[\"date\"] \n",
    "for vn in macvar.columns.values[1:]:\n",
    "    macvarst = pd.concat([macvarst, selectStationaySeries(macvar[vn])], axis=1)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa307b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# macvarst = macvar\n",
    "# lag = [1]\n",
    "\n",
    "# for i in lag:\n",
    "#     if adfuller(macvar[\"ir\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"ir{i}\"] = macvar[\"ir\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"ir\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"ir{i}_2\"] = macvar[\"ir\"].pct_change(i).diff()\n",
    "\n",
    "#     if adfuller(macvar[\"exr\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"exr{i}\"] = macvar[\"exr\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"exr\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"exr{i}_2\"] = macvar[\"exr\"].pct_change(i).diff()\n",
    "\n",
    "#     if adfuller(macvar[\"cpi\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"cpi{i}\"] = macvar[\"cpi\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"cpi\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"cpi{i}_2\"] = macvar[\"cpi\"].pct_change(i).diff()\n",
    "\n",
    "#     if adfuller(macvar[\"gdp\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"gdp{i}\"] = macvar[\"gdp\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"gdp\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"gdp{i}_2\"] = macvar[\"gdp\"].pct_change(i).diff()\n",
    "\n",
    "#     if adfuller(macvar[\"rs\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"rs{i}\"] = macvar[\"rs\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"rs\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"rs{i}_2\"] = macvar[\"rs\"].pct_change(i).diff()\n",
    "\n",
    "#     if adfuller(macvar[\"pda\"].pct_change(i).dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"pda{i}\"] = macvar[\"pda\"].pct_change(i)\n",
    "#     if adfuller(macvar[\"pda\"].pct_change(i).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "#         macvarst[f\"pda{i}_2\"] = macvar[\"pda\"].pct_change(i).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64493c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "month = [int(my_str.split(\"-\")[1]) for my_str in macvarst[\"date\"].values]\n",
    "quater = [(m - 1) // 3 + 1 for m in month]\n",
    "monthdummies = pd.get_dummies(month, prefix=\"month\")\n",
    "quaterdummies = pd.get_dummies(quater, prefix=\"quater\")\n",
    "dummies = pd.concat([monthdummies, quaterdummies], axis=1)\n",
    "#macvarst  =  pd.concat([macvarst,dummies],axis =1)\n",
    "#macvarst = pd.concat([macvarst, monthdummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecea94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# skip reading\n",
    "# macvarsta = macvarst[\n",
    "#     macvarst.columns.difference([\"ir\", \"exr\", \"cpi\", \"gdp\", \"rs\", \"pda\"])\n",
    "# ]\n",
    "# macvarsta = macvarst[macvarst.columns.difference(['ir','exr','cpi','gdp','rs','pda','loghpidiff'])]\n",
    "\n",
    "macvarsta = macvarst  \n",
    "matrixmac = macvarsta.corr().round(2)\n",
    "macdata = macvarsta.loc[\n",
    "    (\"2019-12\" > macvarsta[\"date\"]) & (macvarsta[\"date\"] > \"2000-12\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2469873",
   "metadata": {},
   "outputs": [],
   "source": [
    "macvarsta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train = macdata.loc[:, macdata.columns != \"date\"]\n",
    "poly= PolynomialFeatures(degree=2)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#X_train_poly = poly.fit_transform(X_train)\n",
    "#X_train_std = scaler.fit_transform(X_train_poly)\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf4689",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train_std)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.9) + 1\n",
    "print(d)\n",
    "\n",
    "pca_d = PCA(n_components=d)\n",
    "pca_d.fit(X_train)\n",
    "X_pca_d = pca_d.transform(X_train)\n",
    "PCnames = []\n",
    "for i in range(1, d + 1, 1):\n",
    "    PCnames.append(f\"PC{i}\")\n",
    "pcadf = pd.DataFrame(X_pca_d, columns=PCnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afee3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_avg = pd.concat(\n",
    "    [\n",
    "        pcadf,\n",
    "        agg[\"mean\"]\n",
    "        .loc[(\"2019-12\" > agg[\"Description\"]) & (agg[\"Description\"] > \"2001-01\")]\n",
    "        .reset_index(drop=True),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "# skip reading\n",
    "paneldata = []\n",
    "lst = [\"postcode\", \"date\", \"logHPI\", \"difflogHPI\"]\n",
    "for key in DataFrameDict_postcode.keys():\n",
    "    df = pd.DataFrame([], columns=lst)\n",
    "    df[\"date\"] = DataFrameDict_postcode[key][\"value_at_date\"].loc[\n",
    "        (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "        & (DataFrameDict_postcode[key][\"value_at_date\"] > \"2001-01\")\n",
    "    ]\n",
    "    df[\"logHPI\"] = np.log2(\n",
    "        DataFrameDict_postcode[key][\"Hedonic Home Value Index\"].loc[\n",
    "            (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "            & (DataFrameDict_postcode[key][\"value_at_date\"] > \"2000-12\")\n",
    "        ]\n",
    "    )\n",
    "    df[\"postcode\"] = key\n",
    "    df[\"difflogHPI\"] = (\n",
    "        (\n",
    "            np.log2(\n",
    "                DataFrameDict_postcode[key][\"Hedonic Home Value Index\"].loc[\n",
    "                    (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "                    & (DataFrameDict_postcode[key][\"value_at_date\"] > \"2000-12\")\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        .diff(1)\n",
    "        .dropna()\n",
    "    )\n",
    "    data = pd.concat([df.reset_index(drop=True), pca_avg], axis=1)\n",
    "    paneldata.append(data)\n",
    "paneldf = pd.concat(paneldata, ignore_index=True)\n",
    "paneldf[\"date\"] = pd.to_datetime(paneldf[\"date\"])\n",
    "panelData = paneldf.set_index([\"postcode\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e50e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrameDict_PC = {elem: pd.DataFrame for elem in UniquePostcode}\n",
    "lst = [\"postcode\", \"date\", \"logHPI\", \"difflogHPI\"]\n",
    "# Calling DataFrame constructor on list\n",
    "for key in DataFrameDict_PC.keys():\n",
    "    df = pd.DataFrame([], columns=lst)\n",
    "    df[\"date\"] = DataFrameDict_postcode[key][\"value_at_date\"].loc[\n",
    "        (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "        & (DataFrameDict_postcode[key][\"value_at_date\"] > \"2001-01\")\n",
    "    ]\n",
    "    df[\"logHPI\"] = np.log2(\n",
    "        DataFrameDict_postcode[key][\"Hedonic Home Value Index\"].loc[\n",
    "            (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "            & (DataFrameDict_postcode[key][\"value_at_date\"] > \"2000-12\")\n",
    "        ]\n",
    "    )\n",
    "    df[\"postcode\"] = key\n",
    "    df[\"difflogHPI\"] = (\n",
    "        (\n",
    "            np.log2(\n",
    "                DataFrameDict_postcode[key][\"Hedonic Home Value Index\"].loc[\n",
    "                    (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "                    & (DataFrameDict_postcode[key][\"value_at_date\"] > \"2000-12\")\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        .diff(1)\n",
    "        .dropna()\n",
    "    )\n",
    "    data = pd.concat([df.reset_index(drop=True), pca_avg], axis=1)\n",
    "    DataFrameDict_PC[key] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda367d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PCnames = []\n",
    "for i in range(1, d + 1, 1):\n",
    "    PCnames.append(f\"PC{i}\")\n",
    "PCnames.append(\"mean\")\n",
    "df = pd.DataFrame(pca_avg, columns=PCnames)\n",
    "\n",
    "nobs = 28  # post-covid data is used as the test data set\n",
    "df_train, df_test = df[0:-nobs], df[-nobs:]\n",
    "# Check size\n",
    "\n",
    "start_point = df_train.shape[0]\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "model = VAR(df_train)\n",
    "for i in [1,2,3, 4, 6, 7, 8]:\n",
    "    result = model.fit(i)\n",
    "    print(\"Lag Order =\", i)\n",
    "    print(\"AIC : \", result.aic)\n",
    "    print(\"BIC : \", result.bic)\n",
    "    print(\"FPE : \", result.fpe)\n",
    "    print(\"HQIC: \", result.hqic, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512be90e-320e-4989-bcaf-3042036324e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_fitted = model.fit(2)\n",
    "model_fitted.summary()\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "out = durbin_watson(model_fitted.resid)\n",
    "\n",
    "\n",
    "for col, val in zip(df.columns, out):\n",
    "    print(col, \":\", round(val, 2))\n",
    "\n",
    "lag_order = model_fitted.k_ar\n",
    "forecast_input = df_train.values[-lag_order:]\n",
    "\n",
    "df_forecast = []\n",
    "for i in range(1, nobs + 1, 1):\n",
    "    forecast_input = df.values[\n",
    "        -lag_order + start_point + i - 1 : start_point + i - 1,\n",
    "    ]\n",
    "    df_forecast.append(\n",
    "        model_fitted.forecast(y=forecast_input, steps=1)[\n",
    "            0,\n",
    "        ][-1]\n",
    "    )\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(df_forecast, index=df.index[-nobs:], columns=[\"forecast\"])\n",
    "\n",
    "forecast_input_lt = df_train.values[-lag_order:]\n",
    "fc = model_fitted.forecast(y=forecast_input_lt, steps=nobs)\n",
    "\n",
    "df_results_lt = pd.DataFrame(\n",
    "    fc, index=df.index[-nobs:], columns=df.columns + \"forecast\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9488e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7918d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_frame = pd.DataFrame(\n",
    "    agg[\"Description\"]\n",
    "    .loc[(\"2019-12\" > agg[\"Description\"]) & (agg[\"Description\"] > \"2001-01\")]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "x = date_frame[-nobs:][\"Description\"]\n",
    "color1 = \"#0085c3\"\n",
    "color2 = \"#7ab800\"\n",
    "color3 = \"#dc5034\"\n",
    "y = df_results[\"forecast\"]\n",
    "y2 = df_test[\"mean\"][-nobs:]\n",
    "y3 = savgol_filter(df_results_lt[\"meanforecast\"], 5, 3)\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, y2, marker=\"o\", color=color1, label=\"Average HPI Growth Rate\")\n",
    "ax.plot(\n",
    "    x, y, marker=\"o\", ls=\"--\", color=color2, label=\"Average HPI Growth Rate Forecast\"\n",
    ")\n",
    "ax.plot(x, y3, ls=\"-.\", color=color3, label=\"Average HPI Growth Rate Forecast (LT)\")\n",
    "\n",
    "ax.grid(ls=\":\", color=\"gray\", alpha=0.6)\n",
    "\n",
    "ax.legend(loc=\"upper left\", fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063a624-b8c3-4dc8-b71e-59f8ef3612cc",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Mixed frequency data\n",
    "- Introducing data at smaller scales with lower frequencies (e.g. annual) \n",
    "- Finding the principal components -> new risk factors at smaller scales\n",
    "- New VAR models: average growth rate at smaller scales (annual)\n",
    "- Combining average growth rates from VAR models\n",
    "- Using risk factors(OLS) to explain excess house price growth rate\n",
    "- Downsampling macroeconomic data (monthly) to annual date to build VAR model\n",
    "- Deleting new risk factors when multicollinearity arises (?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d2033-38bd-4f19-bd52-5cf5b0030094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def nan_dum(df):\n",
    "    df1_ohe = df\n",
    "    nan_df = df1_ohe.loc[:, df1_ohe.columns.str.endswith(\"_nan\")]\n",
    "    pattern = \"^([^_]*)_\"\n",
    "    regex = re.compile(pattern)\n",
    "    for index in df1_ohe.index:\n",
    "        for col_nan in nan_df.columns:\n",
    "            if df1_ohe.loc[index, col_nan] == 1:\n",
    "                col_id = regex.search(col_nan).group(1)\n",
    "                targets = df1_ohe.columns[df1_ohe.columns.str.startswith(col_id + \"_\")]\n",
    "                df1_ohe.loc[index, targets] = np.nan\n",
    "    df1_ohe.drop(\n",
    "        df1_ohe.columns[df1_ohe.columns.str.endswith(\"_nan\")], axis=1, inplace=True\n",
    "    )\n",
    "    return df1_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e5d1c-3360-4bc9-af63-7e8da80f7591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def person_cat(postcode, lst):\n",
    "    df_tmp = pd.DataFrame([], columns=lst)\n",
    "    for key in DataFrameDict_HILDA.keys():\n",
    "        lst0 = list(set(lst) - set(DataFrameDict_HILDAp[key].columns))\n",
    "        lst1 = list(set(lst) - set(lst0))\n",
    "        df = (\n",
    "            DataFrameDict_HILDAp[key][lst1]\n",
    "            .loc[DataFrameDict_HILDAp[key][\"hhpcode\"] == postcode]\n",
    "            .astype(str)\n",
    "        )\n",
    "        df[\"date\"] = str(key)\n",
    "        df_tmp = pd.concat([df_tmp, df], axis=0, ignore_index=True)\n",
    "    df_dummy = pd.get_dummies(df_tmp, columns=df_tmp.columns[:-1], dummy_na=True)\n",
    "    dfHILDA = df_dummy.groupby(\"date\").mean().reset_index()\n",
    "    return nan_dum(dfHILDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efc757-a18f-4738-b85a-c7d049b41220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def house_cat(postcode, lst):\n",
    "    df_tmp = pd.DataFrame([], columns=lst)\n",
    "    for key in DataFrameDict_HILDA.keys():\n",
    "        lst0 = list(set(lst) - set(DataFrameDict_HILDA[key].columns))\n",
    "        lst1 = list(set(lst) - set(lst0))\n",
    "        df = (\n",
    "            DataFrameDict_HILDA[key][lst1]\n",
    "            .loc[DataFrameDict_HILDA[key][\"hhpcode\"] == postcode]\n",
    "            .astype(str)\n",
    "        )\n",
    "        df[\"date\"] = str(key)\n",
    "        df_tmp = pd.concat([df_tmp, df], axis=0, ignore_index=True)\n",
    "    df_dummy = pd.get_dummies(df_tmp, columns=df_tmp.columns[:-1], dummy_na=True)\n",
    "    dfHILDA = df_dummy.groupby(\"date\").mean().reset_index()\n",
    "    return nan_dum(dfHILDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def house_cat(postcode, lst):\n",
    "    df_tmp = pd.DataFrame([], columns=lst)\n",
    "    for key in DataFrameDict_HILDA.keys():\n",
    "        lst0 = list(set(lst) - set(DataFrameDict_HILDA[key].columns))\n",
    "        lst1 = list(set(lst) - set(lst0))\n",
    "        df = (\n",
    "            DataFrameDict_HILDA[key][lst1]\n",
    "            .loc[DataFrameDict_HILDA[key][\"hhpcode\"] == postcode]\n",
    "            .astype(str)\n",
    "        )\n",
    "        df[\"date\"] = str(key)\n",
    "        df_tmp = pd.concat([df_tmp, df], axis=0, ignore_index=True)\n",
    "    dfHILDA = df_tmp.groupby(\"date\").mean().reset_index()\n",
    "    return nan_dum(dfHILDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7b44c-0409-47fa-b32d-d21743f8acf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def person_num(postcode, lst):\n",
    "    df_tmp = pd.DataFrame([], columns=lst)\n",
    "    for key in DataFrameDict_HILDA.keys():\n",
    "        lst0 = list(set(lst) - set(DataFrameDict_HILDAp[key].columns))\n",
    "        lst1 = list(set(lst) - set(lst0))\n",
    "        df = DataFrameDict_HILDAp[key][lst1].loc[\n",
    "            DataFrameDict_HILDAp[key][\"hhpcode\"] == postcode\n",
    "        ]\n",
    "        df[\"date\"] = str(key)\n",
    "        df_tmp = pd.concat([df_tmp, df], axis=0, ignore_index=True)\n",
    "    dfHILDA = df_tmp.groupby(\"date\").agg({lambda x: x.mean(skipna=False)})\n",
    "    dfHILDA.columns = lst\n",
    "    return dfHILDA.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e000b9-48b0-4c88-b7ee-8f85b0bcbec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def house_num(postcode, lst):\n",
    "    df_tmp = pd.DataFrame([], columns=lst)\n",
    "    for key in DataFrameDict_HILDA.keys():\n",
    "        lst0 = list(set(lst) - set(DataFrameDict_HILDA[key].columns))\n",
    "        lst1 = list(set(lst) - set(lst0))\n",
    "        df = DataFrameDict_HILDA[key][lst1].loc[\n",
    "            DataFrameDict_HILDA[key][\"hhpcode\"] == postcode\n",
    "        ]\n",
    "        df[\"date\"] = str(key)\n",
    "        df_tmp = pd.concat([df_tmp, df], axis=0, ignore_index=True)\n",
    "    dfHILDA = df_tmp.groupby(\"date\").agg({lambda x: x.mean(skipna=False)})\n",
    "    dfHILDA.columns = lst\n",
    "    return dfHILDA.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b564a-0f3b-4ee3-a37f-98e2c2142fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lst1 variables in HOUSE files\n",
    "# lst2 variables in PERSON files\n",
    "def HILDAext_cat(postcode, lst1, lst2):\n",
    "    dfHILDA1 = house_cat(postcode, lst1)\n",
    "    dfHILDA1[\"date\"] = pd.to_datetime(dfHILDA1[\"date\"]) + pd.offsets.YearEnd()\n",
    "    dfHILDA2 = person_cat(postcode, lst2)\n",
    "    dfHILDA2[\"date\"] = pd.to_datetime(dfHILDA2[\"date\"]) + pd.offsets.YearEnd()\n",
    "    dfHILDA = pd.merge(dfHILDA1, dfHILDA2, on=\"date\", how=\"left\")\n",
    "    return dfHILDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c3f95-6793-4d95-b8c7-cb2211dcd4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def HILDAext_num(postcode, lst1, lst2):\n",
    "    dfHILDA1 = house_num(postcode, lst1)\n",
    "    dfHILDA1[\"date\"] = pd.to_datetime(dfHILDA1[\"date\"]) + pd.offsets.YearEnd()\n",
    "    dfHILDA2 = person_num(postcode, lst2)\n",
    "    dfHILDA2[\"date\"] = pd.to_datetime(dfHILDA2[\"date\"]) + pd.offsets.YearEnd()\n",
    "    dfHILDA = pd.merge(dfHILDA1, dfHILDA2, on=\"date\", how=\"left\")\n",
    "    return dfHILDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e5705-616e-48bd-b130-fe2187824bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60b38b-edc6-49e3-b0d0-117e215c7dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_age = [f'hgdob{i}' for i in range(1, 21)]\n",
    "\n",
    "def HILDAext_age(postcode):    \n",
    "    df_tmp = pd.DataFrame()\n",
    "    for key in DataFrameDict_HILDA.keys():\n",
    "        df = DataFrameDict_HILDA[key][lst_age].loc[DataFrameDict_HILDA[key][\"hhpcode\"] == postcode]\n",
    "        df[\"date\"] = str(key)\n",
    "        df = df.melt(id_vars='date', var_name='Indicator name', value_name='birth_date').sort_values('date', ascending=False).reset_index(drop=True)\n",
    "        df['birthDate'] = pd.to_datetime(df['birth_date'].replace(r'-','na',regex=True),errors = 'coerce')\n",
    "        df['year'] = pd.DatetimeIndex(df['birthDate']).year\n",
    "        df['age'] = key +1 - df['year']\n",
    "        df_tmp = pd.concat([df_tmp, df[['date','age']]], axis=0, ignore_index=True)\n",
    "    f = {'age': ['median', 'std', q1, q3,'count']} \n",
    "    dfHILDAage = df_tmp.groupby('date').agg(f)\n",
    "    dfHILDAage = dfHILDAage.set_axis(dfHILDAage.columns.map(''.join), axis=1, inplace=False).reset_index()\n",
    "    dfHILDAage[\"date\"] = pd.to_datetime(dfHILDAage[\"date\"]) + pd.offsets.YearEnd()\n",
    "    return dfHILDAage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9848e-0585-41c2-82cc-1d425d94119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HILDAext_age(postcode)\n",
    "#HILDAext_age1(postcode)\n",
    "#which shows the household and enumerated dataset are different sources of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c4bf8-ebee-42e2-a4aa-e33d24471a10",
   "metadata": {},
   "source": [
    "# Variables from HILDA\n",
    "\n",
    "[ancob] History: Country of birth\n",
    "\n",
    "[edhigh1] History: Highest education level achieved\n",
    "\n",
    "[anatsi]\tHistory: Aboriginal or Torres Strait Islander origin\n",
    "\n",
    "\n",
    "[chkb12]\tCheck B12 employment status\n",
    "\n",
    "[wsce] DV: Current weekly gross wages & salary, all jobs, includes estimated from net ($) [weighted topcode]\n",
    "\n",
    "[baynoa] Total amount held in these accounts combined [weighted topcode]\n",
    "\n",
    "[xpgroci]\tDV: Household weekly expenditure on all groceries [imputed]\n",
    "\n",
    "[hsdebt]\tDV: Total Home Debt ($) [weighted topcode]\n",
    "\n",
    "[hgdob1] to [hgdob20] Date of birth\n",
    "\n",
    "[hgsex]\tSex\n",
    "\n",
    "\n",
    "```[hgyob]\tYear of Birth```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855b142-02e6-4a11-ab2e-009da277cf7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "varnames_catp = [\"ancob\", \"edhigh1\", \"anatsi\", \"chkb12\", \"sex\"]\n",
    "# varnames_nump = [\"wsce\", \"baynoa\"]\n",
    "varnames_nump = [\"wsce\"]\n",
    "varnames_cath = []\n",
    "#varnames_cath = [\"xpgroci\"]\n",
    "varnames_numh = [\"hsdebt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6356899-83af-4f3e-894e-fb1f2a411849",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example\n",
    "In this example, postcode is fixed to be \"2018\". We can see the results of two data extraction functions from HILDA data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5faace-703c-4ceb-9b2a-f0e5372d1c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "HILDAext_num(\"2018\", varnames_numh, varnames_nump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af14cbf6-49e2-41a8-882f-2f08a5be63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HILDAext_cat(\"2018\", varnames_cath, varnames_catp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a44b44-73d6-471b-b614-aec950cf321d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# Residuals_PC = {elem: pd.DataFrame for elem in UniquePostcode}\n",
    "# lst = [\"postcode\", \"date\", \"residuals\"]\n",
    "# for key in Residuals_PC.keys():\n",
    "#     df = pd.DataFrame([], columns=lst)\n",
    "#     df[\"date\"] = DataFrameDict_PC[key].iloc[:, 1]\n",
    "#     df[\"date\"] = pd.to_datetime(df[\"date\"]) + pd.offsets.MonthEnd()\n",
    "#     df[\"postcode\"] = key\n",
    "#     X = DataFrameDict_PC[key].iloc[:, 4:].values\n",
    "#     Y = DataFrameDict_PC[key].iloc[:, 3].values\n",
    "#     X = sm.add_constant(X)\n",
    "#     result = sm.OLS(Y, X).fit()\n",
    "#     df[\"residuals\"] = result.resid\n",
    "#     data = pd.concat([df.reset_index(drop=True), pca_avg], axis=1)\n",
    "#     HILDAextract_num = HILDAext_num(key, varnames_numh, varnames_nump)\n",
    "#     HILDAextract_cat = HILDAext_cat(key, varnames_cath, varnames_catp)\n",
    "#     HILDAextract = pd.merge(HILDAextract_num, HILDAextract_cat, on=\"date\", how=\"left\")\n",
    "#     Residuals_PC[key] = pd.merge(data, HILDAextract, on=\"date\", how=\"left\")\n",
    "#     print(key + \": success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e919ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals_PC['3150']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beadb01a-1fbe-4315-b5b7-5a973f36fd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def HILDA2PC(key):\n",
    "#     df = Residuals_PC[key].interpolate(\"bfill\").interpolate(\"ffill\")\n",
    "#     df = df.set_index(\"date\")\n",
    "#     df1 = df.resample(\"Y\").mean().iloc[:, d + 2 :]\n",
    "#     x = df1.loc[:, :].values\n",
    "#     x = StandardScaler().fit_transform(x)\n",
    "#     pca = PCA()\n",
    "#     pca.fit(x)\n",
    "#     cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "#     d0 = np.argmax(cumsum >= 0.9) + 1\n",
    "#     pca_d = PCA(n_components=d0)\n",
    "#     pca_d.fit(x)\n",
    "#     X_pca_d = pca_d.transform(x)\n",
    "#     PCnames = []\n",
    "#     for i in range(1, d0 + 1, 1):\n",
    "#         PCnames.append(f\"l_PC{i}\")\n",
    "#     pcadf = pd.DataFrame(X_pca_d, columns=PCnames)\n",
    "#     return pcadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc4110-b7f9-4c9c-98bf-20677d84af2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dti = pd.date_range(\"2001-12-31\", periods=19, freq=\"Y\")\n",
    "\n",
    "\n",
    "# def HILDApcavg(key):\n",
    "#     df = Residuals_PC[key].interpolate(\"bfill\").interpolate(\"ffill\")\n",
    "#     df1 = df.set_index(\"date\")\n",
    "#     df2 = df1.resample(\"Y\").mean().iloc[:, :]\n",
    "#     df3 = pd.concat([df2.iloc[:, 0].reset_index(drop=True), HILDA2PC(key)], axis=1)\n",
    "#     return df3.set_index(dti).resample(\"M\").mean().interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85718779-77ba-4bb6-9c03-603413154d0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# l_DataFrameDict_PC = {elem: pd.DataFrame for elem in UniquePostcode}\n",
    "# lst = [\"postcode\", \"date\", \"difflogHPI\"]\n",
    "# # Calling DataFrame constructor on list\n",
    "# for key in l_DataFrameDict_PC.keys():\n",
    "#     df = pd.DataFrame([], columns=lst)\n",
    "#     df[\"date\"] = DataFrameDict_postcode[key][\"value_at_date\"].loc[\n",
    "#         (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "#         & (DataFrameDict_postcode[key][\"value_at_date\"] > \"2001-01\")\n",
    "#     ]\n",
    "#     df[\"postcode\"] = key\n",
    "#     df[\"difflogHPI\"] = (\n",
    "#         (\n",
    "#             np.log2(\n",
    "#                 DataFrameDict_postcode[key][\"Hedonic Home Value Index\"].loc[\n",
    "#                     (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "#                     & (DataFrameDict_postcode[key][\"value_at_date\"] > \"2000-12\")\n",
    "#                 ]\n",
    "#             )\n",
    "#         )\n",
    "#         .diff(1)\n",
    "#         .dropna()\n",
    "#     )\n",
    "#     data = pd.concat([df.reset_index(drop=True), pca_avg], axis=1)\n",
    "#     data[\"date\"] = pd.to_datetime(data[\"date\"]) + pd.offsets.MonthEnd()\n",
    "#     data = data.set_index(\"date\")\n",
    "#     dframe = pd.concat([data, HILDApcavg(key)], axis=1)\n",
    "#     l_DataFrameDict_PC[key] = dframe.interpolate(\"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d1c23-e040-431a-b8a9-6fbb665f574a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# l_DataFrameDict_PC[\"2018\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6386489-6e07-4b18-8dad-97ae424d1bad",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "- This is a 2-level model: more data should be included \n",
    "- Average R-squared increases from 37% to 50% with the introduction of HILDA data.\n",
    "- Next step: cutting down number of parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a065839f-4873-492b-ac02-d4321b0dd8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # New average R-squared after the introduction of HILDA data\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# Rsq = []\n",
    "# for key in l_DataFrameDict_PC.keys():\n",
    "#     X = l_DataFrameDict_PC[key].iloc[:, 2:].values\n",
    "#     Y = l_DataFrameDict_PC[key].iloc[:, 1].values\n",
    "#     X = sm.add_constant(X)\n",
    "#     result = sm.OLS(Y, X).fit()\n",
    "#     Rsq.append(result.rsquared)\n",
    "# from statistics import mean\n",
    "\n",
    "# cleanedRsq = [x for x in Rsq if str(x) != \"nan\"]\n",
    "# mean(cleanedRsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79884e45-6cb0-4592-b2b2-f8b6d7674f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "aveHPI_lv3 = []\n",
    "lst = [\"postcode\", \"sa3\", \"state\", \"logHPI\", \"logHPIdiff\"]\n",
    "# Calling DataFrame constructor on list\n",
    "for key in DataFrameDict_postcode.keys():\n",
    "    df = pd.DataFrame([], columns=lst)\n",
    "    df[\"Description\"] = DataFrameDict_postcode[key][\"value_at_date\"].loc[\n",
    "        (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "        & (DataFrameDict_postcode[key][\"value_at_date\"] > \"1999-12\")\n",
    "    ]\n",
    "    df[\"logHPI\"] = np.log2(\n",
    "        DataFrameDict_postcode[key][\"Hedonic Home Value Index\"].loc[\n",
    "            (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "            & (DataFrameDict_postcode[key][\"value_at_date\"] > \"1999-12\")\n",
    "        ]\n",
    "    )\n",
    "    df[\"postcode\"] = key\n",
    "    df[\"sa3\"] = DataFrameDict_postcode[key][\"sa3_name16\"].loc[\n",
    "        ( DataFrameDict_postcode[key][\"postcode\"] == key)]\n",
    "    df[\"state\"] = DataFrameDict_postcode[key][\"state\"].loc[\n",
    "        ( DataFrameDict_postcode[key][\"postcode\"] == key)]\n",
    "    \n",
    "    df[\"logHPIdiff\"] = df[\"logHPI\"].diff(1)\n",
    "    aveHPI_lv3.append(df)\n",
    "\n",
    "aveHPIdf_lv3 = pd.concat(aveHPI_lv3)\n",
    "aveHPIdf_lv3 = aveHPIdf_lv3.reset_index(drop=True)\n",
    "aveHPIdf_lv3['date'] = pd.to_datetime(aveHPIdf_lv3['Description'],errors = 'coerce')\n",
    "aveHPIdf_lv3['year'] = pd.DatetimeIndex(aveHPIdf_lv3['date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f36b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "aveHPIdf_lv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1_y = aveHPIdf_lv3.groupby([\"year\"])[\"logHPIdiff\"].agg([\"mean\"]).reset_index()\n",
    "avg_1_y['date'] = pd.to_datetime(avg_1_y['year'].apply(str)) + pd.offsets.YearEnd()\n",
    "avg_1_y= avg_1_y.set_index(\"date\")\n",
    "\n",
    "avg_1 = aveHPIdf_lv3.groupby([\"date\"])[\"logHPIdiff\"].agg([\"mean\"]).reset_index()\n",
    "avg_1= avg_1.set_index(\"date\")\n",
    "\n",
    "avg_2 = aveHPIdf_lv3.groupby([\"state\",\"year\"])[\"logHPIdiff\"].agg([\"mean\"]).reset_index()\n",
    "avg_2['date'] = pd.to_datetime(avg_2['year'].apply(str)) + pd.offsets.YearEnd()\n",
    "avg_2= avg_2.set_index(\"date\")\n",
    "\n",
    "\n",
    "avg_3 = aveHPIdf_lv3.groupby([\"sa3\",\"year\"])[\"logHPIdiff\"].agg([\"mean\"]).reset_index()\n",
    "avg_3['date'] = pd.to_datetime(avg_3['year'].apply(str)) + pd.offsets.YearEnd()\n",
    "avg_3= avg_3.set_index(\"date\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eee999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# DataFrameDict_CL = {elem: pd.DataFrame for elem in UniquePostcode}\n",
    "# lst = [\"postcode\",\"sa3\" ,\"state\" , \"date\"]\n",
    "# for key in DataFrameDict_CL.keys():\n",
    "#     df = pd.DataFrame([], columns=lst)\n",
    "#     df[\"date\"] = DataFrameDict_postcode[key][\"value_at_date\"].loc[(\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "#         & (DataFrameDict_postcode[key][\"value_at_date\"] > \"1999-12\")]\n",
    "#     df[\"date\"] =  pd.to_datetime(df[\"date\"])\n",
    "#     df[\"difflogHPI\"] = (\n",
    "#         (\n",
    "#             np.log2(\n",
    "#                 DataFrameDict_postcode[key][\"Hedonic Home Value Index\"].loc[\n",
    "#                     (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "#                     & (DataFrameDict_postcode[key][\"value_at_date\"] > \"2000-12\")\n",
    "#                 ]\n",
    "#             )\n",
    "#         )\n",
    "#         .diff(1)\n",
    "#         .dropna()\n",
    "#     )\n",
    "#     df[\"postcode\"] = key\n",
    "#     sa3name =  DataFrameDict_postcode[key][\"sa3_name16\"].loc[( DataFrameDict_postcode[key][\"postcode\"] == key)].unique()[0]\n",
    "#     statename = DataFrameDict_postcode[key][\"state\"].loc[( DataFrameDict_postcode[key][\"postcode\"] == key)].unique()[0]\n",
    "#     df= df.set_index(\"date\")\n",
    "#     df0 = pd.concat([df,avg_1['mean']], axis=1,join = \"inner\")\n",
    "#     df1 = pd.concat([df0,avg_2['mean'].loc[avg_2['state'] == statename]], axis=1)\n",
    "#     df2 = pd.concat([df1,avg_3['mean'].loc[avg_3['sa3'] == sa3name]], axis=1)\n",
    "#     df_re = df2.drop(columns=df2.columns[[0,1,2]]).drop(df2.index[range(12)], axis=0)\n",
    "#     df_out = pd.DataFrame([])\n",
    "#     df_out['HPI'] = df_re['difflogHPI']\n",
    "#     df_out['l0'] = df_re.iloc[:,1]\n",
    "#     df_out['l1'] = df_re.iloc[:,2].interpolate() - df_re.iloc[:,1].interpolate() \n",
    "#     df_out['l2'] = df_re.iloc[:,3].interpolate() - df_re.iloc[:,2].interpolate()\n",
    "#     df_out['l3'] = df_out['HPI']- df_out['l0']- df_out['l1'] - df_out['l2']\n",
    "#     DataFrameDict_CL[key] = df_out.drop(df2.index[[-1,12]], axis=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9db8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "DataFrameDict_CL = {elem: pd.DataFrame for elem in UniquePostcode}\n",
    "lst = [\"postcode\",\"sa3\" ,\"state\" , \"date\"]\n",
    "for key in DataFrameDict_CL.keys():\n",
    "    df = pd.DataFrame([], columns=lst)\n",
    "    df[\"date\"] = DataFrameDict_postcode[key][\"value_at_date\"].loc[(\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "        & (DataFrameDict_postcode[key][\"value_at_date\"] > \"1999-12\")]\n",
    "    df[\"date\"] =  pd.to_datetime(df[\"date\"])\n",
    "    df[\"difflogHPI\"] = (\n",
    "        (\n",
    "            np.log2(\n",
    "                DataFrameDict_postcode[key][\"Hedonic Home Value Index\"].loc[\n",
    "                    (\"2019-12\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "                    & (DataFrameDict_postcode[key][\"value_at_date\"] > \"2000-12\")\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        .diff(1)\n",
    "        .dropna()\n",
    "    )\n",
    "    df[\"postcode\"] = key\n",
    "    sa3name =  DataFrameDict_postcode[key][\"sa3_name16\"].loc[( DataFrameDict_postcode[key][\"postcode\"] == key)].unique()[0]\n",
    "    statename = DataFrameDict_postcode[key][\"state\"].loc[( DataFrameDict_postcode[key][\"postcode\"] == key)].unique()[0]\n",
    "    df= df.set_index(\"date\")\n",
    "    df0 = pd.concat([df,avg_1['mean']], axis=1,join = \"inner\")\n",
    "    df1 = pd.concat([df0,avg_2['mean'].loc[avg_2['state'] == statename]], axis=1)\n",
    "    df2 = pd.concat([df1,avg_3['mean'].loc[avg_3['sa3'] == sa3name]], axis=1)\n",
    "    df_re = df2.drop(columns=df2.columns[[0,1,2]]).drop(df2.index[range(12)], axis=0)\n",
    "    df_out = pd.DataFrame([])\n",
    "    df_out['HPI'] = df_re['difflogHPI']\n",
    "    df_out['l0'] = df_re.iloc[:,1]\n",
    "    df_out['l1'] = df_re.iloc[:,2].interpolate() - df_re.iloc[:,1].interpolate()\n",
    "    df_out['l2'] = df_re.iloc[:,3].interpolate() - df_re.iloc[:,2].interpolate()\n",
    "    df_out['l3'] = df_out['HPI']- df_out['l0']- df_out['l1'] - df_out['l2'] \n",
    "    DataFrameDict_CL[key] = df_out.drop(df2.index[[-1,12]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58fd28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrameDict_CL['2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebeebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # New average R-squared after the introduction of HILDA data\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# Rsq = []\n",
    "# for key in DataFrameDict_CL.keys():\n",
    "#     X = l_DataFrameDict_PC[key].iloc[:, 2:].values\n",
    "#     Y = DataFrameDict_CL[key].iloc[:,4].values\n",
    "#     X = sm.add_constant(X)\n",
    "#     result = sm.OLS(Y, X).fit()\n",
    "#     Rsq.append(result.rsquared)\n",
    "# from statistics import mean\n",
    "\n",
    "# cleanedRsq = [x for x in Rsq if str(x) != \"nan\"]\n",
    "# mean(cleanedRsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "UniqueNames_Hsa3 = avg_3.sa3.unique()[:-1]\n",
    "DataFrameDict_Hsa3 = {elem: pd.DataFrame for elem in UniqueNames_Hsa3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrameDict_Hsa3.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in DataFrameDict_Hsa3.keys():\n",
    "    lstpcCL= ndatahouses['postcode'].loc[ndatahouses['sa3_name16'] == key].unique()\n",
    "    lst0 = list(set(lstpcCL) - set(HILDAallpc))\n",
    "    lstpc  = list(set(lstpcCL) - set(lst0))\n",
    "    df_tmp = pd.DataFrame([])\n",
    "    for key_pc in lstpc:\n",
    "        HILDAextract_num = HILDAext_num(key_pc, varnames_numh, varnames_nump)\n",
    "        HILDAextract_cat = HILDAext_cat(key_pc, varnames_cath, varnames_catp)\n",
    "        HILDAextract_age = HILDAext_age(key_pc)\n",
    "        HILDAextract = pd.merge(HILDAextract_num, HILDAextract_cat, on=\"date\", how=\"left\")\n",
    "        HILDAextract = pd.merge( HILDAextract , HILDAextract_age, on=\"date\", how=\"left\")\n",
    "        df_tmp = pd.concat([df_tmp, HILDAextract], axis=0, ignore_index=True)\n",
    "    dfHILDA = df_tmp.groupby(\"date\").agg({lambda x: x.mean(skipna=True)})\n",
    "    list1, list2 = zip(*dfHILDA.columns)\n",
    "    dfHILDA.columns = list1\n",
    "    DataFrameDict_Hsa3[key] = dfHILDA.reset_index()    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_of_dfs.pickle', 'wb') as f:\n",
    "    pickle.dump(DataFrameDict_Hsa3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba702f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('dict_of_dfs.pickle', 'rb') as f:\n",
    "#     Test_Dict =  pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46249b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HILDA2PCsa3(key):\n",
    "    df = DataFrameDict_Hsa3[key].interpolate(\"bfill\").interpolate(\"ffill\")\n",
    "    df = df.set_index(\"date\")\n",
    "    df1 = df.resample(\"Y\").mean().iloc[:, :]\n",
    "    x = df1.loc[:, :].values\n",
    "    \n",
    "    vrst = macvar[\"date\"] \n",
    "    for vn in macvar.columns.values[1:]:\n",
    "        macvarst = pd.concat([macvarst, selectStationaySeries(macvar[vn])], axis=1)\n",
    "   \n",
    "    \n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    pca = PCA()\n",
    "    pca.fit(x)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    d0 = np.argmax(cumsum >= 0.9) + 1\n",
    "    pca_d = PCA(n_components=d0)\n",
    "    pca_d.fit(x)\n",
    "    X_pca_d = pca_d.transform(x)\n",
    "    PCnames = []\n",
    "    for i in range(1, d0 + 1, 1):\n",
    "        PCnames.append(f\"l_PC{i}\")\n",
    "    pcadf = pd.DataFrame(X_pca_d, columns=PCnames)\n",
    "    return pcadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc83f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectStationaySeries2(variable_tar):\n",
    "    count1 = np.isinf(variable_tar.pct_change(1)).values.sum()\n",
    "    count2 = np.isinf(variable_tar.pct_change(1)).values.sum()\n",
    "    \n",
    "    if adfuller(variable_tar.interpolate(\"bfill\").interpolate(\"ffill\"))[1] < threshold:\n",
    "            stationary_variable = variable_tar.interpolate(\"bfill\").interpolate(\"ffill\")\n",
    "            suffix =  \"original\"\n",
    "            print(suffix)\n",
    "            return(pd.DataFrame(stationary_variable))\n",
    "    \n",
    "    \n",
    "    elif count1 == 0:\n",
    "        try:\n",
    "            if adfuller(variable_tar.pct_change(1).interpolate(\"bfill\").interpolate(\"ffill\"))[1] < threshold:\n",
    "                stationary_variable = variable_tar.pct_change(1).interpolate(\"bfill\").interpolate(\"ffill\")\n",
    "                suffix = \"1storderdiff\"\n",
    "                print(suffix)\n",
    "                return(pd.DataFrame(stationary_variable))\n",
    "        except np.linalg.LinAlgError as e1:\n",
    "            print('except:', e1)\n",
    "    \n",
    "    \n",
    "    elif count2 == 0:\n",
    "        try: \n",
    "            if adfuller(variable_tar.pct_change(1).diff().dropna(how=\"all\"))[1] < threshold:\n",
    "                stationary_variable = variable_tar.pct_change(1).diff().dropna(how=\"all\")\n",
    "                suffix = \"2ndorderdiff\"\n",
    "                print(suffix)\n",
    "                return(pd.DataFrame(stationary_variable))\n",
    "        except np.linalg.LinAlgError as e1:\n",
    "            print('except:', e1)     \n",
    "    \n",
    "    else:\n",
    "        print('not found')\n",
    "        \n",
    "\n",
    "             \n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrameDict_Hsa3[key].interpolate(\"bfill\").interpolate(\"ffill\")\n",
    "df = df.set_index(\"date\")\n",
    "df1 = df.resample(\"Y\").mean().iloc[:, :]\n",
    "x = df1.loc[:, :].values\n",
    "vrst = pd.DataFrame([])\n",
    "for vn in df1.columns.values[1:]:\n",
    "    vrst= pd.concat([vrst, selectStationaySeries2(df1[vn])], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fcd64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a5d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
