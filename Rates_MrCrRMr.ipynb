{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e14d0db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'EmpiricalCopula' from 'copulae' (/Users/leiflyu/miniforge3/envs/tf/lib/python3.8/site-packages/copulae/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopulae\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopulae\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmpiricalCopula, pseudo_obs\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopulae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_marginal_data\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myfinance\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01myf\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'EmpiricalCopula' from 'copulae' (/Users/leiflyu/miniforge3/envs/tf/lib/python3.8/site-packages/copulae/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "import copulae\n",
    "from copulae import EmpiricalCopula, pseudo_obs\n",
    "from copulae.datasets import load_marginal_data\n",
    "import yfinance as yf\n",
    "from yahoofinancials import YahooFinancials\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import branca\n",
    "from scipy.interpolate import griddata\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import PathPatch\n",
    "from matplotlib.collections import PatchCollection\n",
    "import seaborn as sns\n",
    "import tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321d53da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/pandas/compat/_optional.py:141\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:973\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xlrd'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf05hist.xls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite(r\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf05hist.xls\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries ID\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/pandas/io/excel/_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    481\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/pandas/io/excel/_base.py:1695\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/pandas/io/excel/_xlrd.py:34\u001b[0m, in \u001b[0;36mXlrdReader.__init__\u001b[0;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mReader using xlrd engine.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m{storage_options}\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstall xlrd >= 1.0.0 for Excel support\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxlrd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(filepath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/pandas/compat/_optional.py:144\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd."
     ]
    }
   ],
   "source": [
    "# dvmr: discounted variable mortgage rates\n",
    "url = \"https://www.rba.gov.au/statistics/tables/xls/f05hist.xls\"\n",
    "r = requests.get(url)\n",
    "open('f05hist.xls', 'wb').write(r.content)\n",
    "df = pd.read_excel('f05hist.xls', sheet_name='Data', skiprows=10)\n",
    "df = df.rename(columns={\"Series ID\": \"date\"})\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index('date')\n",
    "df = df.loc[df.index >= '2004-07-01']\n",
    "df_dvmr = df['FILRHLBVD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cr: cash rate\n",
    "url = \"https://www.rba.gov.au/statistics/tables/xls/f01hist.xls\"\n",
    "r = requests.get(url)\n",
    "open('f01hist.xls', 'wb').write(r.content)\n",
    "df = pd.read_excel('f01hist.xls', sheet_name='Data', skiprows=10)\n",
    "df = df.rename(columns={\"Series ID\": \"date\"})\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index('date')\n",
    "df = df.loc[df.index >= '2004-07-01']\n",
    "df_cr = df['FIRMMCRT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nominal_to_effective(nominal_rate, periods_per_year):\n",
    "    \"\"\"\n",
    "    Converts a nominal annual interest rate to an effective annual interest rate\n",
    "    for a given number of compounding periods per year.\n",
    "    \n",
    "    Args:\n",
    "        nominal_rate (float): The nominal annual interest rate (as a decimal).\n",
    "        periods_per_year (int): The number of compounding periods per year.\n",
    "    \n",
    "    Returns:\n",
    "        The effective annual interest rate (as a decimal).\n",
    "    \"\"\"\n",
    "    return ((1 + 0.01* nominal_rate / periods_per_year) ** periods_per_year -1)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c950a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rates = pd.concat([df_cr, df_dvmr],axis=1)\n",
    "\n",
    "df_rates = df_rates.rename(columns={\"FIRMMCRT\": \"Cash Rates\", \"FILRHLBVD\": \"Discounted Lending Rates\"})\n",
    "df_rates['HEAS(PLS)'] = 5.25\n",
    "df_rates.loc['2020-01-31':'2021-12-31', 'HEAS(PLS)'] = 4.5\n",
    "df_rates.loc['2022-01-31':, 'HEAS(PLS)'] = 3.95\n",
    "df_effective_rates = df_rates.apply(lambda x: nominal_to_effective(x, 12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97066d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumprod = (1+df_rates.loc['2021-01-31':'2021-12-31', 'Discounted Lending Rates']/1200).cumprod().values[-1]\n",
    "((cumprod)**(1/26)-1)*26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_effective_rates.plot(figsize=(10, 5))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Effective Interest Rates (%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f09cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xls1 = pd.read_excel(\"f01hist.xls\", sheet_name=\"Data\", header=2)\n",
    "xls1 = xls1.drop(xls1.index[0:8], axis=0)\n",
    "xls1[\"Description\"] = pd.to_datetime(xls1[\"Description\"])\n",
    "xls1[\"Description\"] = xls1[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "\n",
    "xls3 = pd.read_excel(\"f11hist-1969-2009.xls\", sheet_name=\"Data\", header=2)\n",
    "xls3 = xls3.drop(xls3.index[0:8], axis=0)\n",
    "xls3[\"Description\"] = pd.to_datetime(xls3[\"Description\"])\n",
    "xls3[\"Description\"] = xls3[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "\n",
    "xls4 = pd.read_excel(\"f11hist.xls\", sheet_name=\"Data\", header=2)\n",
    "xls4 = xls4.drop(xls4.index[0:8], axis=0)\n",
    "xls4[\"Description\"] = pd.to_datetime(xls4[\"Description\"])\n",
    "xls4[\"Description\"] = xls4[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "\n",
    "xls34 = pd.concat([xls3, xls4], axis=0)\n",
    "\n",
    "xls5 = pd.read_excel(\"g01hist.xls\", sheet_name=\"Data\", header=2)\n",
    "xls5 = xls5.drop(xls5.index[0:8], axis=0)\n",
    "xls5[\"Description\"] = pd.to_datetime(xls5[\"Description\"])\n",
    "xls5[\"Description\"] = xls5[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "\n",
    "xls6 = pd.read_excel(\"h01hist.xls\", sheet_name=\"Data\", header=2)\n",
    "xls6 = xls6.drop(xls6.index[0:8], axis=0)\n",
    "xls6[\"Description\"] = pd.to_datetime(xls6[\"Description\"])\n",
    "xls6[\"Description\"] = xls6[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "\n",
    "xls7 = pd.read_excel(\"h03hist.xls\", sheet_name=\"Data\", header=2)\n",
    "xls7 = xls7.drop(xls7.index[0:8], axis=0)\n",
    "xls7[\"Description\"] = pd.to_datetime(xls7[\"Description\"])\n",
    "xls7[\"Description\"] = xls7[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "# Download ASX data from Yahoo Finance\n",
    "\n",
    "xls8 = pd.read_excel(\"f05hist.xls\", sheet_name=\"Data\", header=2)\n",
    "xls8 = xls8.drop(xls8.index[0:8], axis=0)\n",
    "xls8[\"Description\"] = pd.to_datetime(xls8[\"Description\"])\n",
    "xls8[\"Description\"] = xls8[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "\n",
    "ASX200data = yf.download('^AXJO', start='1999-01-01', end='2021-06-01', interval = \"1mo\", progress=False)\n",
    "ASX200data = ASX200data.reset_index()\n",
    "ASX200data[\"Description\"] = pd.to_datetime(ASX200data[\"Date\"])\n",
    "ASX200data[\"Description\"] = ASX200data[\"Description\"].dt.date.apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "\n",
    "macvar = pd.DataFrame().assign(\n",
    "    Description=xls7.loc[\n",
    "        (\"2021-04\" > xls7[\"Description\"]) & (xls7[\"Description\"] > \"2004-06\")\n",
    "    ][\"Description\"]\n",
    ")\n",
    "macvar.reset_index(drop=True, inplace=True)\n",
    "\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls1[[\"Description\", \"Cash Rate Target; monthly average\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls34[[\"Description\", \"AUD/USD Exchange Rate; see notes for further detail.\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls5[[\"Description\", \"Consumer price index; All groups\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls6[[\"Description\", \"Gross domestic product (GDP); Chain volume\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls7[[\"Description\", \"Retail sales; All industries; Current price\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls7[[\"Description\", \"Private dwelling approvals\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    ASX200data[[\"Description\", \"Adj Close\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "macvar = pd.merge(\n",
    "    macvar,\n",
    "    xls8[[\"Description\", \"Lending rates; Housing loans; Banks; Variable; Discounted; Owner-occupier\"]],\n",
    "    on=\"Description\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "\n",
    "macvar = macvar.set_axis(\n",
    "    [\"date\", \"ir\", \"exr\", \"cpi\", \"gdp\", \"rs\", \"pda\", \"asx\",\"dvmr\"], axis=1, inplace=False\n",
    ")\n",
    "macvar[\"ir\"] = macvar[\"ir\"].astype(float, errors=\"raise\")\n",
    "macvar[\"exr\"] = macvar[\"exr\"].astype(float, errors=\"raise\")\n",
    "macvar[\"cpi\"] = macvar[\"cpi\"].astype(float, errors=\"raise\")\n",
    "macvar[\"gdp\"] = macvar[\"gdp\"].astype(float, errors=\"raise\")\n",
    "macvar[\"rs\"] = macvar[\"rs\"].astype(float, errors=\"raise\")\n",
    "macvar[\"pda\"] = macvar[\"pda\"].astype(float, errors=\"raise\")\n",
    "macvar[\"asx\"] = macvar[\"asx\"].astype(float, errors=\"raise\")\n",
    "macvar[\"dvmr\"] = macvar[\"dvmr\"].astype(float, errors=\"raise\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "macvar = macvar.set_index('date')\n",
    "macvar.index = pd.to_datetime(macvar.index)\n",
    "macvar.index = macvar.index.to_period('M')\n",
    "macvar = macvar.resample('Q').mean()\n",
    "\n",
    "macvar['ir'] = macvar['ir'] - 0.09\n",
    "macvar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132acbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "macvar_log = macvar.apply(lambda x: np.log(x))\n",
    "macvar_log = macvar_log.reset_index()\n",
    "\n",
    "macvar_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f573339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold = 0.1\n",
    "\n",
    "def selectStationaySeries(variable_tar):\n",
    "    if adfuller(variable_tar.dropna(how=\"all\"))[1] < threshold:\n",
    "        stationary_variable = variable_tar\n",
    "        suffix =  \"original\"\n",
    "\n",
    "    elif adfuller(variable_tar.diff().dropna(how=\"all\"))[1] < threshold:\n",
    "        stationary_variable = variable_tar.diff().dropna(how=\"all\")\n",
    "        suffix = \"1storderdiff\"\n",
    "    \n",
    "    elif adfuller(variable_tar.diff(2).dropna(how=\"all\"))[1] < threshold:\n",
    "        stationary_variable =variable_tar.diff(2).dropna(how=\"all\")\n",
    "        suffix = \"halfyear\"\n",
    "    \n",
    "        \n",
    "    elif adfuller(variable_tar.diff(4).dropna(how=\"all\"))[1] < threshold:\n",
    "        stationary_variable =variable_tar.diff(4).dropna(how=\"all\")\n",
    "        suffix = \"oneyear\"\n",
    "    \n",
    "    elif adfuller(variable_tar.diff().diff().dropna(how=\"all\"))[1] < threshold:\n",
    "        stationary_variable = variable_tar.diff().diff().dropna(how=\"all\")\n",
    "        suffix = \"2ndorderdiff\"     \n",
    "\n",
    "#     elif adfuller(variable_tar.diff(3).dropna(how=\"all\"))[1] < threshold:\n",
    "#         stationary_variable = variable_tar.pct_change(3).dropna(how=\"all\")\n",
    "#         suffix = \"seasonaldiff\"\n",
    "\n",
    "#     elif adfuller(variable_tar.diff(12).dropna(how=\"all\"))[1] < threshold:\n",
    "#         stationary_variable = variable_tar.pct_change(12).dropna(how=\"all\")\n",
    "#         suffix = \"annualdiff\" \n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"not found\")\n",
    "    return(pd.DataFrame(stationary_variable),suffix)\n",
    "\n",
    "\n",
    "macvarst = pd.DataFrame({})\n",
    "macvarst[\"date\"]  = macvar_log[\"date\"] \n",
    "# macvarst[\"ir\"] = macvarst[\"ir\"].diff()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_way = []\n",
    "for vn in macvar_log.columns.values[2:-1]:\n",
    "    df, way = selectStationaySeries(macvar_log[vn])\n",
    "    print(vn,way)\n",
    "    macvarst = pd.concat([macvarst, df], axis=1)\n",
    "    st_way.append(way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52916fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "macdata = macvarst.loc[(\"2021Q1\" >= macvarst[\"date\"]) & (macvarst[\"date\"] > \"2004Q4\")]\n",
    "\n",
    "X_train = macdata.loc[:, macdata.columns != \"date\"]\n",
    "scaler_pca_nat = StandardScaler()\n",
    "X_train_std = scaler_pca_nat.fit_transform(X_train)\n",
    "# scaler_pca_nat.inverse_transform()\n",
    "\n",
    "pca_nat = PCA()\n",
    "pca_nat.fit(X_train_std)\n",
    "\n",
    "# cumsum = np.cumsum(pca_nat.explained_variance_ratio_)\n",
    "# d0 = np.argmax(cumsum >= 0.9) + 1\n",
    "# pca_nat = PCA(n_components=d0)\n",
    "# pca_nat.fit(X_train_std)\n",
    "X_pca_d = pca_nat.transform(X_train_std)\n",
    "\n",
    "# However, in general, it is not possible to fully recover the original dataset from a PCA-transformed dataset,\n",
    "#as some information is lost during the dimensionality reduction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55903690",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCnames = []\n",
    "\n",
    "d = np.shape(X_pca_d)[1]\n",
    "\n",
    "for i in range(1, d + 1, 1):\n",
    "    PCnames.append(f\"PC{i}\")\n",
    "    \n",
    "PC_nat = pd.DataFrame(X_pca_d, columns=PCnames)\n",
    "\n",
    "PC_nat['date'] = macdata['date'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_fast = macvar_log[[\"ir\",\"dvmr\"]].diff().iloc[2:,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f797216",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_nat = pd.concat([df_fast,PC_nat],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_l0 = PC_nat.set_index('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC_nat = macvarst[macvarst['date']>'2004Q4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DataFrameDict_postcode.pickle', 'rb') as f:\n",
    "     DataFrameDict_postcode =  pickle.load(f) \n",
    "        \n",
    "aveHPI_lv3 = []\n",
    "lst = [\"postcode\", \"sa4\", \"state\", \"logHPI\", \"logHPIdiff\"]\n",
    "# Calling DataFrame constructor on list\n",
    "for key in DataFrameDict_postcode.keys():\n",
    "    df = pd.DataFrame([], columns=lst)\n",
    "    df[\"Description\"] = DataFrameDict_postcode[key][\"value_at_date\"].loc[\n",
    "        (\"2021-06\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "        & (DataFrameDict_postcode[key][\"value_at_date\"] > \"1999-12\")\n",
    "    ]\n",
    "    df[\"logHPI\"] =np.log2(\n",
    "        DataFrameDict_postcode[key][\"Hedonic Home Value Index\"].loc[\n",
    "            (\"2021-06\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "            & (DataFrameDict_postcode[key][\"value_at_date\"] > \"1999-12\")\n",
    "        ]\n",
    "    )\n",
    "    df[\"postcode\"] = key\n",
    "    df[\"sa4\"] = DataFrameDict_postcode[key][\"sa4_name16\"].loc[\n",
    "        ( DataFrameDict_postcode[key][\"postcode\"] == key)]\n",
    "    df[\"state\"] = DataFrameDict_postcode[key][\"state\"].loc[\n",
    "        ( DataFrameDict_postcode[key][\"postcode\"] == key)]\n",
    "    \n",
    "    df[\"logHPIdiff\"] = df[\"logHPI\"].diff(1)\n",
    "    aveHPI_lv3.append(df)\n",
    "\n",
    "aveHPIdf_lv3 = pd.concat(aveHPI_lv3)\n",
    "aveHPIdf_lv3 = aveHPIdf_lv3.reset_index(drop=True)\n",
    "aveHPIdf_lv3['date'] = pd.to_datetime(aveHPIdf_lv3['Description'],errors = 'coerce')\n",
    "aveHPIdf_lv3['year'] = pd.DatetimeIndex(aveHPIdf_lv3['date']).year\n",
    "\n",
    "aveHPIdf_lv3 = aveHPIdf_lv3[aveHPIdf_lv3['date'] > '2000-12-31']\n",
    "\n",
    "\n",
    "\n",
    "avg_1 = aveHPIdf_lv3.groupby([\"date\"])[\"logHPIdiff\"].agg([\"mean\"]).reset_index()\n",
    "avg_1 = avg_1.rename(columns = {'mean':'l0'}).dropna().reset_index(drop = True)\n",
    "\n",
    "sum_1 = aveHPIdf_lv3.groupby([\"date\"])[\"logHPIdiff\"].agg([\"sum\"]).reset_index()\n",
    "sum_1 = sum_1.rename(columns = {'sum':'l0_s'}).dropna().reset_index(drop = True)\n",
    "sum_1 = sum_1.reset_index(drop=True)\n",
    "\n",
    "#sa4 \n",
    "avg_2 = aveHPIdf_lv3.groupby([\"sa4\",\"date\"])[\"logHPIdiff\"].agg([\"mean\"]).reset_index()\n",
    "avg_2 = avg_2.rename(columns = {'mean':'l01'}).dropna().reset_index(drop = True)\n",
    "\n",
    "sum_2 = aveHPIdf_lv3.groupby([\"sa4\",\"date\"])[\"logHPIdiff\"].agg([\"sum\"]).reset_index()\n",
    "sum_2 = sum_2.rename(columns = {'sum':'l01_s'}).dropna().reset_index(drop = True)\n",
    "sum_2 = sum_2.reset_index(drop=True)\n",
    "\n",
    "#postcode\n",
    "avg_3 = aveHPIdf_lv3.groupby([\"postcode\",\"date\"])[\"logHPIdiff\"].agg([\"mean\"]).reset_index()\n",
    "avg_3 = avg_3.rename(columns = {'mean':'l012'}).dropna().reset_index(drop = True)\n",
    "\n",
    "# Set the 'date' column to a datetime format\n",
    "avg_2['date'] = pd.to_datetime(avg_2['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Create a new dataframe with monthly dates from '2000-01-31' to '2021-05-31'\n",
    "date_range = pd.date_range('2004-09-30', '2021-03-31', freq='M')\n",
    "\n",
    "new_df = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Use pivot to reshape the 'avg_2' dataframe\n",
    "pivot_df = avg_2.pivot(index='date', columns='sa4', values='l01')\n",
    "\n",
    "# Merge the 'new_df' and 'pivot_df' dataframes\n",
    "merged_df = new_df.merge(pivot_df, on='date', how='outer')\n",
    "\n",
    "# Sort the columns alphabetically by column name\n",
    "merged_df = merged_df.reindex(sorted(merged_df.columns), axis=1)\n",
    "\n",
    "# Fill missing values with NaN\n",
    "avg_2_pivot = merged_df.fillna(np.nan).drop('date', axis=1)\n",
    "\n",
    "# Set the 'date' column to a datetime format\n",
    "sum_2['date'] = pd.to_datetime(sum_2['date'], format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "new_df = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Use pivot to reshape the 'sum_2' dataframe\n",
    "pivot_df = sum_2.pivot(index='date', columns='sa4', values='l01_s')\n",
    "\n",
    "# Merge the 'new_df' and 'pivot_df' dataframes\n",
    "merged_df = new_df.merge(pivot_df, on='date', how='outer')\n",
    "\n",
    "# Sort the columns alphabetically by column name\n",
    "merged_df = merged_df.reindex(sorted(merged_df.columns), axis=1)\n",
    "\n",
    "# Fill missing values with NaN\n",
    "sum_2_pivot = merged_df.fillna(np.nan).drop('date', axis=1)\n",
    "\n",
    "# Set the 'date' column to a datetime format\n",
    "avg_3['date'] = pd.to_datetime(avg_3['date'], format='%Y-%m-%d')\n",
    "\n",
    "new_df = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Use pivot to reshape the 'avg_3' dataframe\n",
    "pivot_df = avg_3.pivot(index='date', columns='postcode', values='l012')\n",
    "\n",
    "# Merge the 'new_df' and 'pivot_df' dataframes\n",
    "merged_df = new_df.merge(pivot_df, on='date', how='outer')\n",
    "\n",
    "# Sort the columns alphabetically by column name\n",
    "merged_df = merged_df.reindex(sorted(merged_df.columns), axis=1)\n",
    "\n",
    "# Fill missing values with NaN\n",
    "avg_3_pivot = merged_df.fillna(np.nan).drop('date', axis=1)\n",
    "\n",
    "avg1 = avg_1.set_index(\"date\")\n",
    "avg2 = avg_2.set_index(\"date\")\n",
    "avg3 = avg_3.set_index(\"date\")\n",
    "df_dis =  pd.concat([avg1 , avg2.pivot(columns='sa4', values='l01'), avg3.pivot(columns='postcode', values='l012')],axis = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dis.index = df_dis.index.to_period('M')\n",
    "df_dis = df_dis.loc['2005-01': '2021-03'].resample('Q').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e05bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified \n",
    "def transform_df(df0):\n",
    "    df = df0.copy()\n",
    "     # subtract the value of each column in group 3 by the value of the corresponding column in group 2   \n",
    "    for col in avg_3_pivot.columns:\n",
    "        df[col] = df[col].sub(df['l0'], axis=0)\n",
    "    return df.drop(avg_2_pivot, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l = transform_df(df_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fcc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_nat= pd.merge(df_l['l0'], PC_l0, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269780b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_aic = float('inf')\n",
    "best_model = None\n",
    "for i in range(1, 5):\n",
    "    model = VAR(train_data_nat)\n",
    "    results = model.fit(i)\n",
    "    print( results.aic)\n",
    "    if results.aic < best_aic:\n",
    "        best_aic = results.aic\n",
    "        best_model = results\n",
    "print(f'Best model has {best_model.k_ar} lags with AIC of {best_aic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aecafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bic = float('inf')\n",
    "best_model = None\n",
    "for i in range(1, 5):\n",
    "    model = VAR(train_data_nat)\n",
    "    results = model.fit(i)\n",
    "    print( results.bic)\n",
    "    if results.bic < best_bic:\n",
    "        best_bic = results.bic\n",
    "        best_model = results\n",
    "print(f'Best model has {best_model.k_ar} lags with BIC of {best_bic}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5475f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "df_onesteperror = pd.DataFrame({},columns = avg_3_pivot.columns)\n",
    "for pc in avg_3_pivot.columns:\n",
    "    start_period = pd.Period('2005Q1', freq='Q')\n",
    "    end_period = pd.Period('2017Q3', freq='Q')\n",
    "    test_period = pd.Period('2017Q4', freq='Q')\n",
    "    # Define a list to store the test dates\n",
    "    errors = [] \n",
    "    # Repeat the simulation for subsequent test dates\n",
    "    while test_period <=  pd.Period('2021Q1', freq='Q'):\n",
    "        train_data =train_data_nat.loc[start_period:end_period]\n",
    "        model = VAR(train_data)\n",
    "        results = model.fit(1)\n",
    "        X_test = results.forecast(train_data.values[-1:], steps=1)\n",
    "        predicted_l0 = X_test[0][0]\n",
    "        traningY = df_l.loc[start_period:end_period][pc]\n",
    "        trainingdf1 = PC_l0.loc[start_period:end_period].reset_index(drop=True)\n",
    "        Y = traningY.values\n",
    "        X = trainingdf1.values\n",
    "        X_train = sm.add_constant(X)\n",
    "        model_full = sm.OLS(Y,X_train)\n",
    "        reg_full = model_full.fit()\n",
    "        predicted_l2 = reg_full.predict(X_test) \n",
    "        error = df_l.loc[test_period][pc]+train_data_nat.loc[test_period]['l0'] - (predicted_l0 + predicted_l2[0] )\n",
    "        errors.append(error)\n",
    "        start_period += 1\n",
    "        end_period += 1\n",
    "        # Update test period by one month\n",
    "        test_period += 1\n",
    "    df_onesteperror[pc] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada44174",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp = df_onesteperror\n",
    "u = pseudo_obs(df_emp)\n",
    "emp_cop = EmpiricalCopula(u, smoothing=\"beta\")\n",
    "df_vol = EmpiricalCopula.to_marginals(emp_cop.random(10000, seed=10), df_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e81d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simplified_simulation(train_data_nat_,start_period,end_period): \n",
    "#     train_data_nat = train_data_nat_.copy()\n",
    "#     train_data =train_data_nat.loc[start_period:end_period]\n",
    "#     model = VAR(train_data)\n",
    "#     results = model.fit(1)\n",
    "#     X_test = results.forecast(train_data.values[-1:], steps=1)\n",
    "#     random_int_vol = random.randint(0, 9999)\n",
    "#     predicted_l0 = X_test[0][0]\n",
    "#     onesteppres = []\n",
    "#     for pc in avg_3_pivot.columns:\n",
    "#         traningY = df_l.loc[start_period:end_period][pc]\n",
    "#         trainingdf1 = PC_l0.loc[start_period:end_period].reset_index(drop=True)\n",
    "#         Y = traningY.values\n",
    "#         X = trainingdf1.values\n",
    "#         X_train = sm.add_constant(X)\n",
    "#         model_full = sm.OLS(Y,X_train)\n",
    "#         reg_full = model_full.fit()\n",
    "#         predicted_l2 = reg_full.predict(X_test) \n",
    "#         onesteppre =  predicted_l0 + predicted_l2[0] + df_vol.loc[random_int_vol][pc] \n",
    "#         onesteppres.append(onesteppre)\n",
    "#     new_nat_rf = X_test[0]\n",
    "#     new_nat_rf[0] = np.mean(onesteppres)\n",
    "#     return new_nat_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simplified_simulation_lvl2(train_data_nat):\n",
    "#     full_data_nat  = train_data_nat.copy() \n",
    "#     train_data_nat_ = train_data_nat.copy() \n",
    "#     start_period = pd.Period('2007-05', freq='M')\n",
    "#     end_period = pd.Period('2021-05', freq='M')\n",
    "#     test_period = pd.Period('2021-06', freq='M')\n",
    "#     while test_period <  pd.Period('2056-06', freq='M'):\n",
    "#         new_nat_rf_ = simplified_simulation(train_data_nat_,start_period,end_period) \n",
    "#         df_nat_rf = pd.DataFrame([new_nat_rf_], columns =train_data_nat_.columns)\n",
    "#         full_data_nat = pd.concat([full_data_nat, df_nat_rf],axis = 0)\n",
    "#         start_period += 1\n",
    "#         end_period += 1\n",
    "#         # Update test period by one month\n",
    "#         test_period += 1\n",
    "#     return full_data_nat   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285656ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_onesteperror_nat = pd.DataFrame({},columns = avg_3_pivot.columns)\n",
    "\n",
    "#     start_period = pd.Period('2005Q1', freq='Q')\n",
    "#     end_period = pd.Period('2017Q3', freq='Q')\n",
    "#     test_period = pd.Period('2017Q4', freq='Q')\n",
    "#     # Define a list to store the test dates\n",
    "#     errors = [] \n",
    "#     # Repeat the simulation for subsequent test dates\n",
    "#     while test_period <=  pd.Period('2021Q1', freq='Q'):\n",
    "#         train_data =train_data_nat.loc[start_period:end_period]\n",
    "#         model = VAR(train_data)\n",
    "#         results = model.fit(1)\n",
    "#         X_test = results.forecast(train_data.values[-1:], steps=1)\n",
    "#         predicted_l0 = X_test[0][0]\n",
    "#         traningY = df_l.loc[start_period:end_period][pc]\n",
    "#         trainingdf1 = PC_l0.loc[start_period:end_period].reset_index(drop=True)\n",
    "#         Y = traningY.values\n",
    "#         X = trainingdf1.values\n",
    "#         X_train = sm.add_constant(X)\n",
    "#         model_full = sm.OLS(Y,X_train)\n",
    "#         reg_full = model_full.fit()\n",
    "#         predicted_l2 = reg_full.predict(X_test) \n",
    "#         error = df_l.loc[test_period][pc]+train_data_nat.loc[test_period]['l0'] - (predicted_l0 + predicted_l2[0] )\n",
    "#         errors.append(error)\n",
    "#         start_period += 1\n",
    "#         end_period += 1\n",
    "#         # Update test period by one month\n",
    "#         test_period += 1\n",
    "#     df_onesteperror[pc] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d4a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_nat = train_data_nat.values\n",
    "mat_l2 =(df_l.values)[:,1: ]\n",
    "selected_columns = [0,1,2,3,4,7]\n",
    "train_data =mat_nat[- 4*16:,selected_columns]\n",
    "model = VAR(train_data)\n",
    "VAR_nat = model.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf54075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_aic = float('inf')\n",
    "# best_model = None\n",
    "# for i in range(1, 5):\n",
    "#     model = VAR(train_data)\n",
    "#     results = model.fit(i)\n",
    "#     print( results.aic)\n",
    "#     if results.aic < best_aic:\n",
    "#         best_aic = results.aic\n",
    "#         best_model = results\n",
    "# print(f'Best model has {best_model.k_ar} lags with AIC of {best_aic}')\n",
    "\n",
    "# best_bic = float('inf')\n",
    "# best_model = None\n",
    "# for i in range(1, 5):\n",
    "#     model = VAR(train_data)\n",
    "#     results = model.fit(i)\n",
    "#     print( results.bic)\n",
    "#     if results.bic < best_bic:\n",
    "#         best_bic = results.bic\n",
    "#         best_model = results\n",
    "# print(f'Best model has {best_model.k_ar} lags with BIC of {best_bic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e74f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jb_test = sm.stats.stattools.jarque_bera(VAR_nat.resid)\n",
    "print('Jarque-Bera test statistic:', jb_test[0])\n",
    "print('p-value:', jb_test[1])\n",
    "# sns.distplot(VAR_nat.resid[:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aced572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_threshold = 0.05\n",
    "# for i in range(len(VAR_nat.params)):\n",
    "#     for j in range(len(VAR_nat.params[i])):\n",
    "#         if VAR_nat.pvalues[i][j] > p_threshold:\n",
    "#             VAR_nat.params[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3964432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp = pd.DataFrame(VAR_nat.resid)\n",
    "u = pseudo_obs(df_emp)\n",
    "emp_cop = EmpiricalCopula(u, smoothing=\"beta\")\n",
    "df_vol_nat = EmpiricalCopula.to_marginals(emp_cop.random(10000, seed=10), df_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8fe0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mat_nat[- 4*16:,selected_columns[1:]]\n",
    "Dict_reg = {}\n",
    "pc_index = 0 \n",
    "\n",
    "for pc in avg_3_pivot.columns:  \n",
    "    Y = mat_l2[- 4*16:,pc_index]\n",
    "    pc_index = pc_index + 1\n",
    "    X_train = sm.add_constant(X)\n",
    "    model_full = sm.OLS(Y,X_train)\n",
    "    reg_full = model_full.fit()\n",
    "    Dict_reg[pc] =  reg_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53346f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simplified_simulation_lvl1(mat_nat0,mat_l20): \n",
    "#     random_int_vol = random.randint(0, 9999)\n",
    "#     start_period = - 4*16\n",
    "#     train_data =mat_nat0[start_period:,:]\n",
    "#     results = VAR_nat\n",
    "#     X_test = results.forecast(train_data[-1:], steps=1)\n",
    "# #     predicted_l0 = X_test[0][0]\n",
    "    \n",
    "#     mat_pc_nat = mat_nat0[start_period:,1:]\n",
    "\n",
    "#     onesteppres = []\n",
    "#     pc_index = 0 \n",
    "#     for pc in avg_3_pivot.columns:\n",
    "# #       arr_l2 = (df_l[pc].values)[start_period:]\n",
    "# #         Y =mat_l2[start_period:,pc_index]\n",
    "#         pc_index = pc_index + 1\n",
    "\n",
    "# #         X_train = sm.add_constant(X)\n",
    "# #         model_full = sm.OLS(Y,X_train)\n",
    "# #         reg_full = model_full.fit()\n",
    "#         reg_full = Dict_reg[pc] \n",
    "#         predicted_l2 = reg_full.predict(X_test) \n",
    "#         onesteppre =  predicted_l0 + predicted_l2[0] + df_vol.loc[random_int_vol][pc] \n",
    "#         onesteppres.append(onesteppre)\n",
    "   \n",
    "#     idx = np.abs(df_vol_nat.iloc[:,0] -  df_vol.loc[random_int_vol].mean()).argmin()\n",
    "# # Extract the whole row\n",
    "#     closest_row =   df_vol_nat.iloc[idx]\n",
    "#     new_nat_rf = X_test[0] + closest_row.values\n",
    "#     return new_nat_rf, onesteppres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc00502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplified_simulation_lvl1(mat_nat0,mat_l20): \n",
    "    random_int_vol = random.randint(0, 9999)\n",
    "    start_period = - 4*16\n",
    "    train_data = mat_nat0[start_period:,:]\n",
    "##    \n",
    "#     model = VAR(train_data)\n",
    "#     results = model.fit(1)\n",
    "    results = VAR_nat\n",
    "#     X_test = results.forecast(train_data.values[-1:], steps=1)\n",
    "\n",
    "    X_test = results.forecast(train_data[-1:], steps=1)\n",
    "#     predicted_l0 = X_test[0][0]\n",
    "    \n",
    "    mat_pc_nat = mat_nat0[start_period:,1:]\n",
    "\n",
    "    onesteppres = []\n",
    "    pc_index = 0 \n",
    "    for pc in avg_3_pivot.columns:      \n",
    "#       arr_l2 = (df_l[pc].values)[start_period:]\n",
    "#       \n",
    "#         X = mat_pc_nat \n",
    "#         Y = mat_l20[start_period:,pc_index]    \n",
    "        pc_index = pc_index + 1       \n",
    "#         X_train = sm.add_constant(X)\n",
    "#         model_full = sm.OLS(Y,X_train)\n",
    "#         reg_full = model_full.fit()    \n",
    "        reg_full = Dict_reg[pc] \n",
    "        predicted_l2 = reg_full.predict(np.insert(X_test[0][1:],0,1)) \n",
    "#        onesteppre =  predicted_l0 + predicted_l2[0] + df_vol.loc[random_int_vol][pc] \n",
    "        onesteppre =  predicted_l0 + predicted_l2[0] \n",
    "        onesteppres.append(onesteppre)\n",
    "#     idx = np.abs(df_vol_nat.iloc[:,0] -  df_vol.loc[random_int_vol].mean()).argmin()\n",
    "# Extract the whole row\n",
    "#     closest_row =   df_vol_nat.iloc[idx]\n",
    "    new_nat_rf = X_test[0] + df_vol_nat.iloc[random_int_vol]\n",
    "#     new_nat_rf = X_test[0]\n",
    "    return new_nat_rf, onesteppres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02523f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplified_simulation_lvl2(train_data_nat,df_l):\n",
    "    mat_nat_ = train_data_nat.values[- 4*16:,:].copy()\n",
    "    mat_l2_ =(df_l.values)[- 4*16:,1: ].copy()\n",
    "    sim_step = 0\n",
    "    while sim_step < 4*35:\n",
    "        sim_step =  sim_step +1 \n",
    "        new_nat_rf_, onesteppres_ = simplified_simulation_lvl1(mat_nat_,mat_l2_)\n",
    "        mat_nat_ = np.vstack([mat_nat_, new_nat_rf_])\n",
    "        mat_l2_ = np.vstack([mat_l2_,np.array(onesteppres_)])\n",
    "    return mat_nat_[64:], mat_l2_[64:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43533688",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict_sim = {}\n",
    "for i in range(0,10): \n",
    "    mat_nat_, mat_l2_ = simplified_simulation_lvl2(train_data_nat.iloc[:,selected_columns],df_l)\n",
    "    Dict_sim[i] = mat_nat_\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rates_ori(arr,ini):\n",
    "#     arr_ori = []\n",
    "#     arr_ori.append(ini + arr[0])\n",
    "#     for i in range(1, np.shape(arr)[0]):\n",
    "#         arr_ori.append(arr_ori[i-1]+ arr[i])\n",
    "#     return np.exp(arr_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rates_ori2(arr,a,b):\n",
    "#     arr_ori = []\n",
    "#     a0 = arr[0] - a + 2*b\n",
    "#     arr_ori.append(a0)\n",
    "#     a1 = arr[1] - b + 2*arr_ori[0]\n",
    "#     arr_ori.append(a1)\n",
    "#     for i in range(2, np.shape(arr)[0]):\n",
    "#         an = arr[i] - arr_ori[i-2] + 2*arr_ori[i-1]\n",
    "#         arr_ori.append(an)\n",
    "#     return np.exp(arr_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "narray = Dict_sim[2]\n",
    "# Extract the first column and name it narr1\n",
    "narr1 = narray[:, 0]\n",
    "# Extract the rest of the array and name it narr2\n",
    "\n",
    "cumulative_growth = np.exp(sum(narr1))-1\n",
    "cumulative_growth \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ce267",
   "metadata": {},
   "outputs": [],
   "source": [
    "narr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "narr2 = narray[:, 1:]\n",
    "\n",
    "# create a numpy array of zeros with the same number of rows as narr2\n",
    "zeros = np.zeros((narr2.shape[0], 2))\n",
    "\n",
    "# concatenate zeros to the end of narr2 along axis 1\n",
    "narr2 = np.concatenate((narr2, zeros), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0992bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "narr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534af2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var_nat_processed = pca_nat.inverse_transform(narr2)\n",
    "var_nat = scaler_pca_nat.inverse_transform(var_nat_processed)\n",
    "# ###\n",
    "# var_nat = narr2\n",
    "ir = var_nat[:,0] \n",
    "dvmr = var_nat[:,7] \n",
    "ir_ori_ = rates_ori2(ir,macvar_log['ir'].values[-2],macvar_log['ir'].values[-1])\n",
    "dvmr_ori_ = rates_ori(dvmr,macvar_log['dvmr'].values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvmr_ori_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea57087",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_ori_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb878e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8afa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    " macvar_log['ir'].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_gr = []\n",
    "# Dict_exo_rf = {}\n",
    "\n",
    "# for i in range(0,100):\n",
    "#     narray = Dict_sim[i]\n",
    "#     # Extract the first column and name it narr1\n",
    "#     narr1 = narray[:, 0]\n",
    "#     cumulative_growth = np.exp(sum(narr1))-1\n",
    "#     array_gr.append(cumulative_growth)\n",
    "    \n",
    "#     # Extract the rest of the array and name it narr2\n",
    "#     narr2 = narray[:, 1:]\n",
    "#     var_nat_processed = pca_nat.inverse_transform(narr2)\n",
    "#     var_nat = scaler_pca_nat.inverse_transform(var_nat_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
