{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18bd6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import difflib\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "from random import sample\n",
    "import folium\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "import seaborn as sns\n",
    "import statsmodels.tools.tools as sm\n",
    "import statsmodels.base as sb\n",
    "from folium.features import CustomIcon\n",
    "from folium.plugins import FastMarkerCluster, HeatMap, MarkerCluster\n",
    "from linearmodels.panel.model import PooledOLS\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.tools.eval_measures import aic, rmse\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.vector_ar.svar_model import SVAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import yfinance as yf\n",
    "from yahoofinancials import YahooFinancials\n",
    "import pickle\n",
    "import array_to_latex as a2l\n",
    "import markdown\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pdfkit as pdf\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5f0164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the original Data is changed by me. \n",
    "data = pd.read_csv(\n",
    "    \"./markettrends0.csv\",\n",
    "    dtype={\n",
    "        \"state\": \"str\",\n",
    "        \"sa3_name16\": \"str\",\n",
    "        \"sa4_name16\": \"str\",\n",
    "        \"postcode\": \"str\",\n",
    "        \"state\": \"str\",\n",
    "        \"property_type\": \"str\",\n",
    "    },\n",
    ")\n",
    "ndata = data.fillna(\n",
    "    {\"Volume of new rental listings (1 month)\": 0, \"Volume of sales (1 month)\": 0}\n",
    ").dropna(subset=[\"postcode\"])\n",
    "ndatahouses = ndata[:][ndata.property_type == \"Houses\"]\n",
    "ndataunits = ndata[:][ndata.property_type == \"Units\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56f449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DataFrameDict_postcode.pickle', 'rb') as f:\n",
    "     DataFrameDict_postcode =  pickle.load(f) \n",
    "        \n",
    "aveHPI_lv3 = []\n",
    "lst = [\"postcode\", \"sa4\", \"state\", \"logHPI\", \"logHPIdiff\"]\n",
    "# Calling DataFrame constructor on list\n",
    "for key in DataFrameDict_postcode.keys():\n",
    "    df = pd.DataFrame([], columns=lst)\n",
    "    df[\"Description\"] = DataFrameDict_postcode[key][\"value_at_date\"].loc[\n",
    "        (\"2021-06\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "        & (DataFrameDict_postcode[key][\"value_at_date\"] > \"1999-12\")\n",
    "    ]\n",
    "    df[\"logHPI\"] =np.log2(\n",
    "        DataFrameDict_postcode[key][\"Hedonic Home Value Index\"].loc[\n",
    "            (\"2021-06\" > DataFrameDict_postcode[key][\"value_at_date\"])\n",
    "            & (DataFrameDict_postcode[key][\"value_at_date\"] > \"1999-12\")\n",
    "        ]\n",
    "    )\n",
    "    df[\"postcode\"] = key\n",
    "    df[\"sa4\"] = DataFrameDict_postcode[key][\"sa4_name16\"].loc[\n",
    "        ( DataFrameDict_postcode[key][\"postcode\"] == key)]\n",
    "    df[\"state\"] = DataFrameDict_postcode[key][\"state\"].loc[\n",
    "        ( DataFrameDict_postcode[key][\"postcode\"] == key)]\n",
    "    \n",
    "    df[\"logHPIdiff\"] = df[\"logHPI\"].diff(1)\n",
    "    aveHPI_lv3.append(df)\n",
    "\n",
    "aveHPIdf_lv3 = pd.concat(aveHPI_lv3)\n",
    "aveHPIdf_lv3 = aveHPIdf_lv3.reset_index(drop=True)\n",
    "aveHPIdf_lv3['date'] = pd.to_datetime(aveHPIdf_lv3['Description'],errors = 'coerce')\n",
    "aveHPIdf_lv3['year'] = pd.DatetimeIndex(aveHPIdf_lv3['date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40a888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aveHPIdf_lv3 = aveHPIdf_lv3[aveHPIdf_lv3['state'] == 'NSW']\n",
    "aveHPIdf_lv3 = aveHPIdf_lv3[aveHPIdf_lv3['date'] > '2009-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e162fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postcode</th>\n",
       "      <th>sa4</th>\n",
       "      <th>state</th>\n",
       "      <th>logHPI</th>\n",
       "      <th>logHPIdiff</th>\n",
       "      <th>Description</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2018</td>\n",
       "      <td>Sydney - City and Inner South</td>\n",
       "      <td>NSW</td>\n",
       "      <td>6.657473</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2018</td>\n",
       "      <td>Sydney - City and Inner South</td>\n",
       "      <td>NSW</td>\n",
       "      <td>6.676620</td>\n",
       "      <td>0.019147</td>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2018</td>\n",
       "      <td>Sydney - City and Inner South</td>\n",
       "      <td>NSW</td>\n",
       "      <td>6.707935</td>\n",
       "      <td>0.031315</td>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2018</td>\n",
       "      <td>Sydney - City and Inner South</td>\n",
       "      <td>NSW</td>\n",
       "      <td>6.718974</td>\n",
       "      <td>0.011039</td>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2018</td>\n",
       "      <td>Sydney - City and Inner South</td>\n",
       "      <td>NSW</td>\n",
       "      <td>6.730457</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>2010-05-31</td>\n",
       "      <td>2010-05-31</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36889</th>\n",
       "      <td>2737</td>\n",
       "      <td>Murray</td>\n",
       "      <td>NSW</td>\n",
       "      <td>7.612153</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36890</th>\n",
       "      <td>2737</td>\n",
       "      <td>Murray</td>\n",
       "      <td>NSW</td>\n",
       "      <td>7.714129</td>\n",
       "      <td>0.101976</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36891</th>\n",
       "      <td>2737</td>\n",
       "      <td>Murray</td>\n",
       "      <td>NSW</td>\n",
       "      <td>7.715083</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36892</th>\n",
       "      <td>2737</td>\n",
       "      <td>Murray</td>\n",
       "      <td>NSW</td>\n",
       "      <td>7.735679</td>\n",
       "      <td>0.020595</td>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36893</th>\n",
       "      <td>2737</td>\n",
       "      <td>Murray</td>\n",
       "      <td>NSW</td>\n",
       "      <td>7.823007</td>\n",
       "      <td>0.087329</td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19591 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      postcode                            sa4 state    logHPI  logHPIdiff  \\\n",
       "121       2018  Sydney - City and Inner South   NSW  6.657473    0.013617   \n",
       "122       2018  Sydney - City and Inner South   NSW  6.676620    0.019147   \n",
       "123       2018  Sydney - City and Inner South   NSW  6.707935    0.031315   \n",
       "124       2018  Sydney - City and Inner South   NSW  6.718974    0.011039   \n",
       "125       2018  Sydney - City and Inner South   NSW  6.730457    0.011483   \n",
       "...        ...                            ...   ...       ...         ...   \n",
       "36889     2737                         Murray   NSW  7.612153    0.019043   \n",
       "36890     2737                         Murray   NSW  7.714129    0.101976   \n",
       "36891     2737                         Murray   NSW  7.715083    0.000954   \n",
       "36892     2737                         Murray   NSW  7.735679    0.020595   \n",
       "36893     2737                         Murray   NSW  7.823007    0.087329   \n",
       "\n",
       "      Description       date  year  \n",
       "121    2010-01-31 2010-01-31  2010  \n",
       "122    2010-02-28 2010-02-28  2010  \n",
       "123    2010-03-31 2010-03-31  2010  \n",
       "124    2010-04-30 2010-04-30  2010  \n",
       "125    2010-05-31 2010-05-31  2010  \n",
       "...           ...        ...   ...  \n",
       "36889  2021-01-31 2021-01-31  2021  \n",
       "36890  2021-02-28 2021-02-28  2021  \n",
       "36891  2021-03-31 2021-03-31  2021  \n",
       "36892  2021-04-30 2021-04-30  2021  \n",
       "36893  2021-05-31 2021-05-31  2021  \n",
       "\n",
       "[19591 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aveHPIdf_lv3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1069460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NSW(The scope of data is narrowed down to the NSW from the national level)\n",
    "avg_1 = aveHPIdf_lv3.groupby([\"date\"])[\"logHPIdiff\"].agg([\"mean\"]).reset_index()\n",
    "avg_1 = avg_1.rename(columns = {'mean':'l0'}).dropna().reset_index(drop = True)\n",
    "\n",
    "sum_1 = aveHPIdf_lv3.groupby([\"date\"])[\"logHPIdiff\"].agg([\"sum\"]).reset_index()\n",
    "sum_1 = sum_1.rename(columns = {'sum':'l0_s'}).dropna().reset_index(drop = True)\n",
    "sum_1 = sum_1.reset_index(drop=True)\n",
    "\n",
    "#sa4 \n",
    "avg_2 = aveHPIdf_lv3.groupby([\"sa4\",\"date\"])[\"logHPIdiff\"].agg([\"mean\"]).reset_index()\n",
    "avg_2 = avg_2.rename(columns = {'mean':'l01'}).dropna().reset_index(drop = True)\n",
    "\n",
    "sum_2 = aveHPIdf_lv3.groupby([\"sa4\",\"date\"])[\"logHPIdiff\"].agg([\"sum\"]).reset_index()\n",
    "sum_2 = sum_2.rename(columns = {'sum':'l01_s'}).dropna().reset_index(drop = True)\n",
    "sum_2 = sum_2.reset_index(drop=True)\n",
    "\n",
    "#postcode\n",
    "avg_3 = aveHPIdf_lv3.groupby([\"postcode\",\"date\"])[\"logHPIdiff\"].agg([\"mean\"]).reset_index()\n",
    "avg_3 = avg_3.rename(columns = {'mean':'l012'}).dropna().reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f989f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'date' column to a datetime format\n",
    "avg_2['date'] = pd.to_datetime(avg_2['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Create a new dataframe with monthly dates from '2000-01-31' to '2021-05-31'\n",
    "date_range = pd.date_range('2010-01-31', '2021-05-31', freq='M')\n",
    "new_df = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Use pivot to reshape the 'avg_2' dataframe\n",
    "pivot_df = avg_2.pivot(index='date', columns='sa4', values='l01')\n",
    "\n",
    "# Merge the 'new_df' and 'pivot_df' dataframes\n",
    "merged_df = new_df.merge(pivot_df, on='date', how='outer')\n",
    "\n",
    "# Sort the columns alphabetically by column name\n",
    "merged_df = merged_df.reindex(sorted(merged_df.columns), axis=1)\n",
    "\n",
    "# Fill missing values with NaN\n",
    "avg_2_pivot = merged_df.fillna(np.nan).drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0208b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'date' column to a datetime format\n",
    "sum_2['date'] = pd.to_datetime(sum_2['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Create a new dataframe with monthly dates from '2000-01-31' to '2021-05-31'\n",
    "date_range = pd.date_range('2010-01-31', '2021-05-31', freq='M')\n",
    "new_df = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Use pivot to reshape the 'sum_2' dataframe\n",
    "pivot_df = sum_2.pivot(index='date', columns='sa4', values='l01_s')\n",
    "\n",
    "# Merge the 'new_df' and 'pivot_df' dataframes\n",
    "merged_df = new_df.merge(pivot_df, on='date', how='outer')\n",
    "\n",
    "# Sort the columns alphabetically by column name\n",
    "merged_df = merged_df.reindex(sorted(merged_df.columns), axis=1)\n",
    "\n",
    "# Fill missing values with NaN\n",
    "sum_2_pivot = merged_df.fillna(np.nan).drop('date', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d04bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'date' column to a datetime format\n",
    "avg_3['date'] = pd.to_datetime(avg_3['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Create a new dataframe with monthly dates from '2000-01-31' to '2021-05-31'\n",
    "date_range = pd.date_range('2010-01-31', '2021-05-31', freq='M')\n",
    "new_df = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Use pivot to reshape the 'avg_3' dataframe\n",
    "pivot_df = avg_3.pivot(index='date', columns='postcode', values='l012')\n",
    "\n",
    "# Merge the 'new_df' and 'pivot_df' dataframes\n",
    "merged_df = new_df.merge(pivot_df, on='date', how='outer')\n",
    "\n",
    "# Sort the columns alphabetically by column name\n",
    "merged_df = merged_df.reindex(sorted(merged_df.columns), axis=1)\n",
    "\n",
    "# Fill missing values with NaN\n",
    "avg_3_pivot = merged_df.fillna(np.nan).drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1571e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_yt = pd.concat([sum_1, sum_2_pivot, avg_3_pivot], axis=1)\n",
    "\n",
    "# Convert the 'date' column to a datetime type\n",
    "df_yt['date'] = pd.to_datetime(df_yt['date'])\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "df_yt = df_yt.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a5540e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "def base_errors(colname, M):\n",
    "    # Get the data for the specified column\n",
    "    data = df_yt[colname].values\n",
    "    error_df = pd.DataFrame()\n",
    "\n",
    "    # Initialize a matrix to store the forecasts\n",
    "#     errors = np.zeros((N, data.shape[0] - M))\n",
    "\n",
    "    # Loop over the rolling window\n",
    "    for i in range(M, data.shape[0] ):\n",
    "        N = data.shape[0] - M\n",
    "        training_data = data[i-M:i]\n",
    "        test_data = data[i:i+N]\n",
    "\n",
    "        # Fit ARIMA model to training data\n",
    "        model = auto_arima(training_data, d = None, seasonal=False, suppress_warnings=True, error_action=\"ignore\", stepwise=True, trace=False)\n",
    "        arima = ARIMA(training_data, order=model.order)\n",
    "        arima_fit = arima.fit()\n",
    "    \n",
    "        # Make N-step-ahead forecast \n",
    "        forecast = arima_fit.forecast(steps=N)\n",
    "        # Check if the forecast is longer than the test_data\n",
    "        if len(forecast) > len(test_data):\n",
    "        # Calculate the difference between the lengths of the two arrays\n",
    "            difference = len(forecast) - len(test_data)\n",
    "    \n",
    "        # Add the extra values from the forecast array to the end of the test_data array\n",
    "            test_data = np.append(test_data, forecast[-difference:])\n",
    "        error_df['sim'+str(i)] = forecast - test_data\n",
    "        # Calculate average MSE for each series\n",
    "    return  error_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30dcbbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = df_yt.columns\n",
    "M0  = 120\n",
    "Dict_Errors = {}\n",
    "\n",
    "for i, colname in enumerate(colnames):\n",
    "    error_results = base_errors(colname,M0)\n",
    "    Dict_Errors[colname] = error_results\n",
    "    print(f'Progress: {i+1}/{len(colnames)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "10b0dec5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict_Errors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s1/vpd2prqd5fsc16qqsymrc9b00000gn/T/ipykernel_60044/719358476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dict_Errors.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDict_Errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Dict_Errors' is not defined"
     ]
    }
   ],
   "source": [
    "# with open('Dict_Errors.pkl', 'wb') as f:\n",
    "#     pickle.dump(Dict_Errors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "90435f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i, colname in enumerate(colnames):\n",
    "#     error_results = base_errors(colname,M0,N0)\n",
    "#     error_full = error_results.iloc[:, 0:-N0+1]\n",
    "#     # Calculate the RMSE of each column in the error_full dataframe\n",
    "#     rmse = np.sqrt(error_full.apply(lambda x: mean_squared_error(x, np.zeros(len(x))), axis=0))\n",
    "#     # Calculate the average RMSE\n",
    "#     avg_rmse_array[i] = np.mean(rmse)\n",
    "#     errors_1[colname] = error_results.iloc[0, :]\n",
    "#     print(f'Progress: {i+1}/{len(colnames)}')\n",
    "# print(avg_rmse_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0d07958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_base(M, h, Dict_Errors, df_yt):\n",
    "    forecasts = pd.DataFrame({}, columns=df_yt.columns, index=df_yt.index)\n",
    "\n",
    "    # Loop over all dataframes in Dict_Errors\n",
    "    for key in Dict_Errors:\n",
    "        # Find the row in Dict_Errors with index h-1 and extract all values before the last h column\n",
    "        errors = Dict_Errors[key].iloc[h-1, :Dict_Errors[key].shape[1]-h+1]\n",
    "\n",
    "        # Calculate the length of the extracted values from Dict_Errors\n",
    "        length = len(errors)\n",
    "\n",
    "        # Cut df_yt[key] into two parts\n",
    "        first_part = df_yt[key].iloc[:-length]\n",
    "        second_part = df_yt[key].iloc[-length:]\n",
    "\n",
    "        # Add the second part of df_yt[key] and the extracted values from Dict_Errors to the forecast dataframe\n",
    "        forecasts.loc[second_part.index, key] = second_part.values + errors.values\n",
    "\n",
    "\n",
    "    return forecasts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "de40ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first row of each dataframe vbuin Dict_Errors and combine them as new columns in a dataframe called error_1step\n",
    "error_1step = pd.concat([Dict_Errors[key].iloc[0, :] for key in Dict_Errors], axis=1)\n",
    "\n",
    "# Rename the columns of error_1step to match the keys in Dict_Errors\n",
    "error_1step.columns = Dict_Errors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ff07c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the column names of df_yt\n",
    "sa4_lvls= list(avg_2_pivot.columns)\n",
    "postcode_lvls = list(avg_3_pivot.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e903877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_matrix_S(sa4_lvls, postcode_lvls, aveHPIdf_lv3):\n",
    "    # Initialize a matrix of zeros with the same shape as the desired matrix S\n",
    "    S = np.zeros((len(sa4_lvls), len(postcode_lvls)))\n",
    "\n",
    "    # Loop over the rows of S\n",
    "    for i in range(len(sa4_lvls)):\n",
    "        # Get a boolean array indicating which rows of aveHPIdf_lv3 have sa4 equal to sa4_lvls[i]\n",
    "        sa4_match = aveHPIdf_lv3['sa4'] == sa4_lvls[i]\n",
    "\n",
    "        # Loop over the columns of S\n",
    "        for j in range(len(postcode_lvls)):\n",
    "            # Set S[i,j] to 1 if aveHPIdf_lv3['postcode'] is equal to postcode_lvls[j] and sa4_match is True\n",
    "            if sa4_match.any() and (aveHPIdf_lv3['postcode'][sa4_match] == postcode_lvls[j]).any():\n",
    "                S[i,j] = 1\n",
    "    top_row = np.ones((1, len(postcode_lvls)))\n",
    "    bottom_row = np.eye(len(postcode_lvls))\n",
    "    Sfinal = np.vstack([top_row, S, bottom_row])\n",
    "\n",
    "            \n",
    "\n",
    "    # Convert the matrix to a dataframe with the appropriate row and column names\n",
    "#     S = pd.DataFrame(S, index=sa4_lvls, columns=postcode_lvls)\n",
    "\n",
    "    return Sfinal \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83b2f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = generate_matrix_S(sa4_lvls, postcode_lvls, aveHPIdf_lv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "9fa3e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mint_shrink(base_forecasts, base_error_1step):\n",
    "    \"\"\"\n",
    "    Implements the MinT (Shrink) algorithm for forecast reconciliation.\n",
    "    Given a set of N-step-ahead base forecasts and the in-sample one-step-ahead\n",
    "    base forecast errors, returns the reconciled forecasts using the MinT (Shrink) method.\n",
    "    \"\"\"\n",
    "    # Get the number of time series and the forecast horizon\n",
    "\n",
    "\n",
    "    # Compute the empirical covariance matrix of the base forecast errors\n",
    "    Sigma = np.cov(base_error_1step.T)\n",
    "\n",
    "    correlation_matrix = base_error_1step.corr()\n",
    "\n",
    "    # Calculate the variance of the correlation matrix\n",
    "    correlation_matrix_var = correlation_matrix.apply(lambda x: (1 - x**2) / (len(base_error_1step) - 3)).values\n",
    "\n",
    "    # Calculate the sum of the variances of the correlation estimates\n",
    "    variance_sum = np.sum(correlation_matrix_var[np.triu_indices(len(base_error_1step), k=1)])\n",
    "\n",
    "    # Calculate the sum of the squares of the correlation estimates\n",
    "    correlation_sum = np.sum(correlation_matrix.values[np.triu_indices(len(base_error_1step), k=1)]**2)\n",
    "\n",
    "    # Calculate the estimator for lambda_D\n",
    "    lambda_D_hat = variance_sum / correlation_sum\n",
    "\n",
    "    W_h = lambda_D_hat* D + (1 - lambda_D_hat) * Sigma\n",
    "\n",
    "    m_star = S.shape[0] - S.shape[1]\n",
    "\n",
    "    C = S[:len(sa4_lvls)+1,: ]\n",
    "\n",
    "    U = np.hstack([np.identity(m_star), -C]).transpose()\n",
    "\n",
    "    J = np.concatenate((np.zeros(( S.shape[1], m_star)), np.eye(S.shape[1])), axis=1)\n",
    "\n",
    "    J_W_U = np.dot(np.dot(J, W_h), U)\n",
    "\n",
    "    UWUinv = np.linalg.inv(np.dot(np.dot(U.transpose(), W_h),U) )\n",
    "\n",
    "    J_W_UUWUinv =  np.dot(J_W_U,UWUinv )\n",
    "\n",
    "    coefficient = np.dot(S, J - np.dot(J_W_UUWUinv ,U.transpose()))\n",
    "    \n",
    "    reconciled_forecasts =np.dot(coefficient,base_forecasts.values.transpose())\n",
    "    return reconciled_forecasts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "48fa466d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/vpd2prqd5fsc16qqsymrc9b00000gn/T/ipykernel_83240/3473400932.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  Dict_recon_Errors[key] = Dict_recon_Errors[key].append(new_df, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "Dict_recon_Errors = {key: pd.DataFrame() for key in Dict_Errors}\n",
    "\n",
    "for h in range(1, len(df_yt) - M0 + 1):\n",
    "    mat = mint_shrink(forecast_base(M0, h, Dict_Errors, df_yt).dropna(), error_1step)\n",
    "    i = 0\n",
    "    for key in  Dict_recon_Errors.keys():\n",
    "        forecasts = mat[i,:] \n",
    "        length = len(forecasts)\n",
    "        testset = df_yt[key].iloc[-length:]\n",
    "        new_row = (forecasts  -   testset).values\n",
    "        new_row_length = len(new_row)\n",
    "        if new_row_length < len(df_yt) - M0 :\n",
    "            missing_cols = len(df_yt) - M0  - new_row_length\n",
    "            new_row = np.append(new_row, np.full(missing_cols, np.nan))\n",
    "        new_df = pd.DataFrame([new_row], columns=df_yt.index[-len(df_yt) + M0 :])\n",
    "\n",
    "# concatenate the new dataframe to the original dataframe\n",
    "        Dict_recon_Errors[key] = Dict_recon_Errors[key].append(new_df, ignore_index=True)\n",
    "        i = i + 1\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "24b3068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict_recon_Errors ['l0_s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "b0e7ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict_Errors ['l0_s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "3a5e5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def error_average(Dict_Name, h, Level):\n",
    "    # check if level is valid\n",
    "    if Level not in ['High', 'Mid', 'Low']:\n",
    "        raise ValueError(\"Level must be 'High', 'Mid', or 'Low'.\")\n",
    "\n",
    "    # extract column names based on level\n",
    "    if Level == 'Low':\n",
    "        col_names = avg_3_pivot.columns\n",
    "    elif Level == 'Mid':\n",
    "        col_names = avg_2_pivot.columns\n",
    "    else:\n",
    "        col_names = ['l0_s']\n",
    "\n",
    "    # extract dataframes from Dict_Name using column names as keys\n",
    "    dfs = []\n",
    "    for col in col_names:\n",
    "        try:\n",
    "            df = Dict_Name[col]\n",
    "            dfs.append(df)\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Could not find dataframe for key {col}.\")\n",
    "\n",
    "    # calculate RMSE for each dataframe and find average by row\n",
    "    rmse_by_row = []\n",
    "    for df in dfs:\n",
    "        df_error = df.iloc[0: h+1, 0:len(df_yt)-M-h].copy()\n",
    "        # calculate RMSE by row\n",
    "        rmse_by_row.append(np.sqrt(np.mean((df_error.values)**2)))\n",
    "\n",
    "    # calculate average of average RMSE by row\n",
    "    if rmse_by_row:\n",
    "        average_rmse = np.mean([np.mean(rmse) for rmse in rmse_by_row])\n",
    "    else:\n",
    "        average_rmse = np.nan\n",
    "\n",
    "    return average_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "ac7b94c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022902698117970154"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_average(Dict_Errors, 1, 'Low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "78b4f317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020416014767204886"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_average(Dict_recon_Errors, 1, 'Low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "0544acd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02469242993098904"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_average(Dict_Errors, 3, 'Low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "9175e49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023136563534317752"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_average(Dict_recon_Errors, 3, 'Low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "f0c0cdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025182141192336114"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_average(Dict_Errors, 12, 'Low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "63241199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025213541403191108"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_average(Dict_recon_Errors, 12, 'Low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "id": "1e013a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store the values for the two lines\n",
    "base_forecast = []\n",
    "reconciled_forecast = []\n",
    "\n",
    "# Iterate over the range of h values from 1 to 12\n",
    "for i in range(1, 13):\n",
    "    # Calculate the values for the two lines using the error_average() function\n",
    "    base_forecast.append(error_average(Dict_Errors, i, 'Low'))\n",
    "    reconciled_forecast.append(error_average(Dict_recon_Errors, i, 'Low'))\n",
    "\n",
    "# Plot the two lines using the plot() function from matplotlib\n",
    "plt.plot(range(1, 13), base_forecast, label=\"base forecast\")\n",
    "plt.plot(range(1, 13), reconciled_forecast, label=\"reconciled forecast\")\n",
    "\n",
    "# Add labels for the x and y axes\n",
    "plt.xlabel(\"h\")\n",
    "plt.ylabel(\"Error Average\")\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063772ac",
   "metadata": {},
   "source": [
    "# Hierarchical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b757b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg1 = avg_1.set_index(\"date\")\n",
    "avg2 = avg_2.set_index(\"date\")\n",
    "avg3 = avg_3.set_index(\"date\")\n",
    "df_dis =  pd.concat([avg1 , avg2.pivot(columns='sa4', values='l01'), avg3.pivot(columns='postcode', values='l012')],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea2db7de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transform_df(df_dis):\n",
    "    # create a copy of df_dis to avoid modifying the original dataframe\n",
    "    df_transformed = df_dis.copy()\n",
    "     # subtract the value of each column in group 3 by the value of the corresponding column in group 2\n",
    "    for col in avg_3_pivot.columns:\n",
    "        sa4 = aveHPIdf_lv3.loc[aveHPIdf_lv3['postcode'] == col]['sa4'].iloc[0]\n",
    "        df_transformed[col] = df_transformed[col].sub(df_transformed[sa4], axis=0)\n",
    "    # subtract the value under column 'l0' from columns in group 2\n",
    "    df_transformed[avg_2_pivot.columns] = df_transformed[avg_2_pivot.columns].sub(df_transformed['l0'], axis=0)\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0005937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform_df(df_transformed):\n",
    "    \n",
    "    # create a copy of df_transformed to avoid modifying the original dataframe\n",
    "    df_dis = df_transformed.copy()\n",
    "    \n",
    "    # add the value under column 'l0' back to columns in group 2\n",
    "    df_dis[avg_2_pivot.columns] = df_dis[avg_2_pivot.columns].add(df_dis['l0'], axis=0)\n",
    "    \n",
    "    # add the value of each column in group 3 back to the value of the corresponding column in group 2\n",
    "    for col in avg_3_pivot.columns:\n",
    "        sa4 = aveHPIdf_lv3.loc[aveHPIdf_lv3['postcode'] == col]['sa4'].iloc[0]\n",
    "        df_dis[col] = df_dis[col].add(df_dis[ sa4], axis=0)\n",
    "    \n",
    "    return df_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ce198282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l = transform_df(df_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6af402d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dict_strt2Arimas.pickle\", \"wb\") as file:\n",
    "     pickle.dump(Dict_strt2Arimas, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d8116bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_len = 17\n",
    "\n",
    "# Create an empty dictionary to store the columns\n",
    "columns = {}\n",
    "\n",
    "# Loop through the column names and create an empty list for each one\n",
    "for col_name in hm_names:\n",
    "    columns[col_name] = []\n",
    "\n",
    "# Populate the columns with the desired length (17)\n",
    "for i in range(df_len):\n",
    "    for col_name in hm_names:\n",
    "        columns[col_name].append(None)\n",
    "\n",
    "# Create the dataframe from the dictionary of columns\n",
    "new_df = pd.DataFrame(columns)\n",
    "\n",
    "\n",
    "# Loop through each row index in new_df\n",
    "for R in range(0,17):\n",
    "    # Loop through each column name in new_df\n",
    "    for key in new_df.columns:\n",
    "        # Get the corresponding ARIMA model from Dict_strt2Arimas and predict one period\n",
    "        arima_model = Dict_strt2Arimas[R + 120][key]['Model']\n",
    "        prediction = arima_model.predict(n_periods=1, return_conf_int=False)[0]\n",
    "        # Set the value in new_df to the predicted value\n",
    "        new_df.loc[R, key] = prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dfa5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5193d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_errors_2(colname, M):\n",
    "    if colname == 'l0' or colname in avg_2_pivot.columns: \n",
    "    # Get the data for the specified column\n",
    "        data = df_l[colname].values\n",
    "        error_df = pd.DataFrame()\n",
    "\n",
    "    # Initialize a matrix to store the forecasts\n",
    "#     errors = np.zeros((N, data.shape[0] - M))\n",
    "\n",
    "    # Loop over the rolling window\n",
    "        for i in range(M, data.shape[0]):\n",
    "            N = data.shape[0] - M\n",
    "            training_data = data[i-M:i]\n",
    "            test_data = data[i:i+N]\n",
    "\n",
    "        # Fit ARIMA model to training data\n",
    "            model = auto_arima(training_data, d = None, seasonal=False, suppress_warnings=True, error_action=\"ignore\", stepwise=True, trace=False)\n",
    "            arima = ARIMA(training_data, order=model.order)\n",
    "            arima_fit = arima.fit()\n",
    "    \n",
    "        # Make N-step-ahead forecast \n",
    "            forecast = arima_fit.forecast(steps=N)\n",
    "        # Check if the forecast is longer than the test_data\n",
    "            if len(forecast) > len(test_data):\n",
    "        # Calculate the difference between the lengths of the two arrays\n",
    "                difference = len(forecast) - len(test_data)\n",
    "    \n",
    "        # Add the extra values from the forecast array to the end of the test_data array\n",
    "                test_data = np.append(test_data, forecast[-difference:])\n",
    "            error_df['sim'+str(i)] = forecast - test_data\n",
    "        # Calculate average MSE for each series\n",
    "        return  error_df\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a8bd3b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = df_l.columns\n",
    "M0  = 120\n",
    "Dict_Errors_Hi = {}\n",
    "\n",
    "for i, colname in enumerate(colnames):\n",
    "    error_results = base_errors_2(colname,M0)\n",
    "    Dict_Errors_Hi[colname] = error_results\n",
    "    print(f'Progress: {i+1}/{len(colnames)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ad60d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Dict_Errors_Hi.pickle\", \"wb\") as file:\n",
    "#     pickle.dump(Dict_Errors_Hi, file)\n",
    "\n",
    "with open(\"Dict_Errors_Hi.pickle\", \"rb\") as file:\n",
    "     Dict_Errors_Hi = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b413249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of keys to delete\n",
    "keys_to_delete = []\n",
    "\n",
    "# Iterate over the keys in Dict_Errors_Hi and identify any null dataframes\n",
    "for key in Dict_Errors_Hi:\n",
    "    if Dict_Errors_Hi[key] is None or Dict_Errors_Hi[key].empty:\n",
    "        keys_to_delete.append(key)\n",
    "\n",
    "# Delete the identified keys from Dict_Errors_Hi\n",
    "for key in keys_to_delete:\n",
    "    del Dict_Errors_Hi[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa385ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_base_2(M, h, Dict_Errors_Hi, df_l):\n",
    "    forecasts = pd.DataFrame({}, columns=df_l.columns, index=df_l.index)\n",
    "\n",
    "    # Loop over all dataframes in Dict_Errors\n",
    "    for key in Dict_Errors_Hi:\n",
    "        # Find the row in Dict_Errors with index h-1 and extract all values before the last h column\n",
    "        errors = Dict_Errors_Hi[key].iloc[h-1, :Dict_Errors_Hi[key].shape[1]-h+1]\n",
    "\n",
    "        # Calculate the length of the extracted values from Dict_Errors\n",
    "        length = len(errors)\n",
    "\n",
    "        # Cut df_yt[key] into two parts\n",
    "        first_part = df_l[key].iloc[:-length]\n",
    "        second_part = df_l[key].iloc[-length:]\n",
    "\n",
    "        # Add the second part of df_yt[key] and the extracted values from Dict_Errors to the forecast dataframe\n",
    "        forecasts.loc[second_part.index, key] = second_part.values + errors.values\n",
    "\n",
    "\n",
    "    return forecasts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c775302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "M0=120\n",
    "ARIMAforecasts = forecast_base_2(M0, 1, Dict_Errors_Hi, df_l)\n",
    "ARIMAforecasts = ARIMAforecasts.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58e25dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def ini_onestepforecast_hi(colname, M):\n",
    "    if colname in avg_3_pivot.columns: \n",
    "    # Get the data for the specified column\n",
    "        sa4 = aveHPIdf_lv3.loc[aveHPIdf_lv3['postcode'] == colname]['sa4'].iloc[0]\n",
    "        results_array = np.empty((df_l.shape[0]-M,)) \n",
    "        for i in range(M, df_l.shape[0]):\n",
    "            N = data.shape[0] - M\n",
    "            train_start = i - M\n",
    "            train_end = i - 1\n",
    "            test_start = i\n",
    "            test_end = i + N\n",
    "            train_X = df_l[['l0', sa4]][train_start:train_end+1].values\n",
    "            train_Y = df_l[colname][train_start:train_end+1].values\n",
    "            model = LinearRegression().fit(train_X, train_Y)           \n",
    "            test_X =np.array(ARIMAforecasts.iloc[test_start][['l0', sa4]]).reshape(1,-1)\n",
    "            test_Y_pred = model.predict(test_X)\n",
    "            results_array[i-M] = test_Y_pred \n",
    "        # Calculate average MSE for each series\n",
    "        return  results_array\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "856c3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onestep = ARIMAforecasts.copy().dropna(how='all').reset_index(drop = True)\n",
    "# Print the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e23f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in avg_3_pivot.columns:\n",
    "    \n",
    "    # Generate one-step forecasts using ini_onestepforecast_hi\n",
    "    forecasts = ini_onestepforecast_hi(col, M0)\n",
    "    \n",
    "    # Add the forecasts to the ARIMAforecasts dataframe\n",
    "    df_onestep[col] = forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c569ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l0</th>\n",
       "      <th>Capital Region</th>\n",
       "      <th>Central Coast</th>\n",
       "      <th>Central West</th>\n",
       "      <th>Coffs Harbour - Grafton</th>\n",
       "      <th>Far West and Orana</th>\n",
       "      <th>Hunter Valley exc Newcastle</th>\n",
       "      <th>Illawarra</th>\n",
       "      <th>Mid North Coast</th>\n",
       "      <th>Murray</th>\n",
       "      <th>...</th>\n",
       "      <th>2760</th>\n",
       "      <th>2763</th>\n",
       "      <th>2770</th>\n",
       "      <th>2774</th>\n",
       "      <th>2782</th>\n",
       "      <th>2795</th>\n",
       "      <th>2800</th>\n",
       "      <th>2829</th>\n",
       "      <th>2830</th>\n",
       "      <th>2871</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018238</td>\n",
       "      <td>-0.011204</td>\n",
       "      <td>-0.004856</td>\n",
       "      <td>-0.009592</td>\n",
       "      <td>-0.004333</td>\n",
       "      <td>-0.033101</td>\n",
       "      <td>-0.010765</td>\n",
       "      <td>-0.003221</td>\n",
       "      <td>-0.00496</td>\n",
       "      <td>-0.010723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>-0.003165</td>\n",
       "      <td>-0.005123</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.012183</td>\n",
       "      <td>0.002332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013868</td>\n",
       "      <td>-0.007653</td>\n",
       "      <td>-0.00521</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>-0.003672</td>\n",
       "      <td>-0.01691</td>\n",
       "      <td>-0.010988</td>\n",
       "      <td>-0.002102</td>\n",
       "      <td>-0.003652</td>\n",
       "      <td>-0.0119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-0.001769</td>\n",
       "      <td>-0.003846</td>\n",
       "      <td>-0.004101</td>\n",
       "      <td>-0.004238</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>0.008340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017186</td>\n",
       "      <td>-0.013381</td>\n",
       "      <td>-0.002929</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>-0.001577</td>\n",
       "      <td>-0.009151</td>\n",
       "      <td>-0.001857</td>\n",
       "      <td>-0.003598</td>\n",
       "      <td>-0.005672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.003158</td>\n",
       "      <td>-0.005275</td>\n",
       "      <td>-0.003806</td>\n",
       "      <td>-0.004006</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>-0.008144</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008829</td>\n",
       "      <td>-0.007429</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>-0.007211</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>-0.007146</td>\n",
       "      <td>-0.000451</td>\n",
       "      <td>-0.009982</td>\n",
       "      <td>-0.007409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>-0.001362</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>-0.001174</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>-0.006530</td>\n",
       "      <td>-0.000339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00442</td>\n",
       "      <td>-0.007048</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>-0.002125</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-0.003564</td>\n",
       "      <td>-0.004851</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>-0.005407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>-0.001348</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>-0.001305</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>-0.001217</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.000984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.005089</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>-0.000896</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.008307</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>-0.004994</td>\n",
       "      <td>0.00186</td>\n",
       "      <td>-0.001819</td>\n",
       "      <td>-0.001284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>-0.001654</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>-0.002064</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>-0.002190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.00319</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.004207</td>\n",
       "      <td>0.005526</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>-0.003466</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>-0.004375</td>\n",
       "      <td>-0.001467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>-0.000933</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>-0.002376</td>\n",
       "      <td>0.001484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.004459</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.002621</td>\n",
       "      <td>0.016155</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>0.030885</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>0.004609</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000432</td>\n",
       "      <td>-0.000822</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>-0.005722</td>\n",
       "      <td>-0.003631</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>-0.015647</td>\n",
       "      <td>0.009353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.002355</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>0.014616</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.008673</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000923</td>\n",
       "      <td>-0.001533</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>-0.005188</td>\n",
       "      <td>-0.003352</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.002286</td>\n",
       "      <td>0.008540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.006539</td>\n",
       "      <td>-0.001266</td>\n",
       "      <td>0.005127</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.008493</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000765</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>-0.003124</td>\n",
       "      <td>-0.002604</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>-0.003219</td>\n",
       "      <td>0.005727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.006576</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.005852</td>\n",
       "      <td>0.004857</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>0.007936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>-0.001709</td>\n",
       "      <td>-0.002465</td>\n",
       "      <td>-0.003482</td>\n",
       "      <td>-0.003697</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>-0.007916</td>\n",
       "      <td>0.007180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.016466</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>-0.001686</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>-0.000827</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>0.009446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>-0.002757</td>\n",
       "      <td>-0.004807</td>\n",
       "      <td>-0.003774</td>\n",
       "      <td>-0.005199</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>-0.008464</td>\n",
       "      <td>0.008973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.01856</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>-0.003743</td>\n",
       "      <td>-0.005492</td>\n",
       "      <td>-0.003703</td>\n",
       "      <td>-0.005105</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>-0.010013</td>\n",
       "      <td>0.008808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.012557</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.00305</td>\n",
       "      <td>0.010422</td>\n",
       "      <td>-0.010213</td>\n",
       "      <td>-0.000884</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.004585</td>\n",
       "      <td>-0.00524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>-0.002038</td>\n",
       "      <td>-0.003214</td>\n",
       "      <td>-0.003128</td>\n",
       "      <td>-0.003564</td>\n",
       "      <td>-0.001154</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.006693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.034146</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>-0.005007</td>\n",
       "      <td>0.00428</td>\n",
       "      <td>-0.002609</td>\n",
       "      <td>-0.010784</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004646</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>-0.006954</td>\n",
       "      <td>-0.009155</td>\n",
       "      <td>-0.003033</td>\n",
       "      <td>-0.005152</td>\n",
       "      <td>0.011221</td>\n",
       "      <td>-0.016571</td>\n",
       "      <td>0.008185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.047558</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>-0.012681</td>\n",
       "      <td>-0.005939</td>\n",
       "      <td>-0.003031</td>\n",
       "      <td>-0.017378</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>-0.004612</td>\n",
       "      <td>0.00078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>-0.000206</td>\n",
       "      <td>-0.011535</td>\n",
       "      <td>-0.009908</td>\n",
       "      <td>-0.002593</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>0.007342</td>\n",
       "      <td>-0.022012</td>\n",
       "      <td>0.003138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.027011</td>\n",
       "      <td>-0.005375</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>-0.01592</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-0.00104</td>\n",
       "      <td>-0.014871</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>-0.001226</td>\n",
       "      <td>-0.005378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>-0.005316</td>\n",
       "      <td>-0.004697</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>-0.011268</td>\n",
       "      <td>-0.003304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows Ã— 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          l0 Capital Region Central Coast Central West  \\\n",
       "0   0.018238      -0.011204     -0.004856    -0.009592   \n",
       "1   0.013868      -0.007653      -0.00521     0.002512   \n",
       "2   0.017186      -0.013381     -0.002929    -0.000656   \n",
       "3   0.008829      -0.007429     -0.000915    -0.007211   \n",
       "4    0.00442      -0.007048      0.000536    -0.002125   \n",
       "5  -0.005089       0.000611     -0.000896    -0.000021   \n",
       "6   -0.00319       0.005282      0.000415     0.004207   \n",
       "7  -0.004459       0.003672      0.002621     0.016155   \n",
       "8  -0.002355       0.006772      0.004402     0.014616   \n",
       "9   0.002889       0.003538      0.004494     0.006539   \n",
       "10  0.010941       0.006576      0.004871     0.003405   \n",
       "11  0.016466       0.004072      0.004819     0.002565   \n",
       "12   0.01856       0.006505      0.004153     0.002242   \n",
       "13  0.012557       0.012987      0.003189      0.00305   \n",
       "14  0.034146       0.002156      0.005621    -0.005007   \n",
       "15  0.047558      -0.010184      0.003235    -0.012681   \n",
       "16  0.027011      -0.005375      0.005767     -0.01592   \n",
       "\n",
       "   Coffs Harbour - Grafton Far West and Orana Hunter Valley exc Newcastle  \\\n",
       "0                -0.004333          -0.033101                   -0.010765   \n",
       "1                -0.003672           -0.01691                   -0.010988   \n",
       "2                 0.002967          -0.001577                   -0.009151   \n",
       "3                 0.002153           0.003554                   -0.007146   \n",
       "4                -0.000112          -0.003564                   -0.004851   \n",
       "5                -0.008307          -0.000912                   -0.004994   \n",
       "6                 0.005526           0.009382                   -0.003466   \n",
       "7                -0.000133           0.030885                   -0.000563   \n",
       "8                 0.005754           0.008673                    0.002854   \n",
       "9                -0.001266           0.005127                    0.006574   \n",
       "10                0.005852           0.004857                    0.003109   \n",
       "11               -0.001686           0.000595                    0.001938   \n",
       "12                0.004856           0.001027                    0.003974   \n",
       "13                0.010422          -0.010213                   -0.000884   \n",
       "14                 0.00428          -0.002609                   -0.010784   \n",
       "15               -0.005939          -0.003031                   -0.017378   \n",
       "16                0.000119           -0.00104                   -0.014871   \n",
       "\n",
       "   Illawarra Mid North Coast    Murray  ...      2760      2763      2770  \\\n",
       "0  -0.003221        -0.00496 -0.010723  ...  0.001310  0.000398 -0.000398   \n",
       "1  -0.002102       -0.003652   -0.0119  ... -0.000563  0.000151 -0.000151   \n",
       "2  -0.001857       -0.003598 -0.005672  ...  0.001136 -0.000053  0.000053   \n",
       "3  -0.000451       -0.009982 -0.007409  ...  0.001874 -0.001362  0.001362   \n",
       "4   0.000749        0.001658 -0.005407  ...  0.003942 -0.001348  0.001348   \n",
       "5    0.00186       -0.001819 -0.001284  ...  0.001177 -0.001654  0.001654   \n",
       "6   0.004988       -0.004375 -0.001467  ...  0.000161 -0.000818  0.000818   \n",
       "7   0.004609        0.001489  0.000046  ... -0.000432 -0.000822  0.000822   \n",
       "8   0.003617        0.001858  0.002251  ... -0.000923 -0.001533  0.001533   \n",
       "9   0.000856        0.008493 -0.000556  ... -0.000389 -0.000765  0.000765   \n",
       "10  0.000826       -0.000262  0.007936  ...  0.001979 -0.000054  0.000054   \n",
       "11 -0.000827        0.004425  0.009446  ...  0.001126  0.000237 -0.000237   \n",
       "12  0.003198        0.000404  0.002536  ...  0.001834  0.000447 -0.000447   \n",
       "13  0.001894        0.004585  -0.00524  ...  0.001297 -0.000072  0.000072   \n",
       "14  0.000984        0.004605  0.001911  ...  0.004646 -0.000164  0.000164   \n",
       "15 -0.000181       -0.004612   0.00078  ...  0.008273  0.000206 -0.000206   \n",
       "16  0.000719       -0.001226 -0.005378  ...  0.002996 -0.000168  0.000168   \n",
       "\n",
       "        2774      2782      2795      2800      2829      2830      2871  \n",
       "0  -0.003165 -0.005123  0.000534 -0.002867  0.000765  0.012183  0.002332  \n",
       "1  -0.001769 -0.003846 -0.004101 -0.004238  0.002461  0.003886  0.008340  \n",
       "2  -0.003158 -0.005275 -0.003806 -0.004006  0.005762 -0.008144  0.007812  \n",
       "3  -0.001600 -0.002300  0.001512 -0.001174  0.003872 -0.006530 -0.000339  \n",
       "4  -0.001305 -0.000494  0.000233 -0.001217 -0.000148  0.001265  0.000984  \n",
       "5   0.001722  0.002563  0.001501  0.000688 -0.002064  0.005321 -0.002190  \n",
       "6   0.001720  0.001375 -0.000933 -0.000552  0.000418 -0.002376  0.001484  \n",
       "7   0.002327  0.002104 -0.005722 -0.003631  0.004548 -0.015647  0.009353  \n",
       "8   0.002074  0.001379 -0.005188 -0.003352  0.000018 -0.002286  0.008540  \n",
       "9   0.000713 -0.000268 -0.003124 -0.002604  0.000654 -0.003219  0.005727  \n",
       "10 -0.001709 -0.002465 -0.003482 -0.003697  0.003405 -0.007916  0.007180  \n",
       "11 -0.002757 -0.004807 -0.003774 -0.005199  0.002912 -0.008464  0.008973  \n",
       "12 -0.003743 -0.005492 -0.003703 -0.005105  0.002048 -0.010013  0.008808  \n",
       "13 -0.002038 -0.003214 -0.003128 -0.003564 -0.001154  0.001283  0.006693  \n",
       "14 -0.006954 -0.009155 -0.003033 -0.005152  0.011221 -0.016571  0.008185  \n",
       "15 -0.011535 -0.009908 -0.002593 -0.000544  0.007342 -0.022012  0.003138  \n",
       "16 -0.005316 -0.004697  0.001347  0.001957  0.003952 -0.011268 -0.003304  \n",
       "\n",
       "[17 rows x 172 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bdd0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onestep_ori = reverse_transform_df(df_onestep)\n",
    "df_onestep_ori = df_onestep_ori.filter(avg_3_pivot.columns, axis=1)\n",
    "df_true = df_dis.copy().reset_index(drop = True)\n",
    "df_true = df_true.tail(df_onestep_ori.shape[0]).reset_index(drop = True)\n",
    "error_copula = (df_onestep_ori - df_true).dropna(axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d60e756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copulae\n",
    "from copulae import EmpiricalCopula, pseudo_obs\n",
    "from copulae.datasets import load_marginal_data\n",
    "df  = error_copula\n",
    "\n",
    "df_emp = df\n",
    "u = pseudo_obs(df)\n",
    "emp_cop = EmpiricalCopula(u, smoothing=\"none\")\n",
    "df_vol = EmpiricalCopula.to_marginals(emp_cop.random(2000, seed=10), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cada8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_u =S[:S.shape[0]- S.shape[1],:]\n",
    "S_u = S_u / np.sum(S_u, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2326df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2key(colname,ARIMAModel_Dict):\n",
    "    key_to_find = colname\n",
    "\n",
    "# Convert the keys of the dictionary to a list\n",
    "    keys_list = list(ARIMAModel_Dict.keys())\n",
    "\n",
    "# Use the index() method of the list to get the position of the key\n",
    "    if key_to_find in keys_list:\n",
    "        key_position = keys_list.index(key_to_find)\n",
    "        return key_position    \n",
    "    else:\n",
    "        print(\"Key {} is not present in the dictionary\".format(key_to_find))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9dfd7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ll = df_l.copy().reset_index(drop = True)\n",
    "# Combine the dataframes\n",
    "combined_df = pd.concat([avg1, avg_2_pivot], axis=1)\n",
    "hm_names = list(combined_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75f8360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARIMA_generator(M,i,colname):\n",
    "    data = df_l[colname].values\n",
    "    training_data_ARMA = data[i-M:i]\n",
    "    arima_model = auto_arima(training_data_ARMA , seasonal=False, suppress_warnings=True)\n",
    "    return arima_model,training_data_ARMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bbb3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strtpoint2ARIMAModel_Dict_init(M,i):\n",
    "    ARIMAModel_Dict = {}\n",
    "    for col in hm_names:\n",
    "        model,training_data = ARIMA_generator(M,i,col)\n",
    "        ARIMAModel_Dict[col] = {'Model':model, 'Data':  training_data }   \n",
    "    return ARIMAModel_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39bd6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARIMA_modifier(arr_newdata):\n",
    "    arr = []\n",
    "    for i in range(0,len(arr_newdata)):\n",
    "        key = list(df_l.columns)[i]\n",
    "        ARIMAModel_Dict[key]['Model'] = ARIMAModel_Dict[key]['Model'].update(arr_newdata[i])\n",
    "        l_new =ARIMAModel_Dict[key]['Model'].predict(n_periods=1, return_conf_int=False)[0]\n",
    "        arr.append(l_new)\n",
    "    return  arr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1615,
   "id": "b620e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 120\n",
    "# M0  = 120\n",
    "# ARIMAModel_Dict0 = strtpoint2ARIMAModel_Dict_init(M0,i)\n",
    "\n",
    "# arr_of_arrs_of_arrs = []\n",
    "# for sim in range (0,Sim_Times):\n",
    "    \n",
    "#     ARIMAModel_Dict = ARIMAModel_Dict0.copy()\n",
    "    \n",
    "#     arr_of_arrs = []\n",
    "#     for steps in range (0, df_l.shape[0] - i):\n",
    "#         l_before_aj = []\n",
    "#         random_int = random.randint(0, 1999)\n",
    "#         for key in ARIMAModel_Dict.keys():\n",
    "#             model = ARIMAModel_Dict[key]['Model']\n",
    "#             l_before_aj.append(model.predict(n_periods=1, return_conf_int=False)[0])\n",
    "            \n",
    "#         arr = []\n",
    "#         for postcode in list(avg_3_pivot.columns):\n",
    "#             sa4 = aveHPIdf_lv3.loc[aveHPIdf_lv3['postcode'] == postcode]['sa4'].iloc[0]\n",
    "#             N = df_ll.shape[0] - M\n",
    "#             train_start = i - M\n",
    "#             train_end = i - 1\n",
    "#             test_start = i\n",
    "#             train_X = df_ll.loc[train_start:train_end+1,['l0', sa4]].values\n",
    "#             train_Y = df_ll.loc[train_start:train_end+1,postcode]\n",
    "#             model = LinearRegression().fit(train_X, train_Y)           \n",
    "#             test_X = np.array([l_before_aj[0],l_before_aj[str2key(sa4)]]).reshape(1,-1)\n",
    "#             test_Y_pred = model.predict(test_X) \n",
    "#             l012_next = test_Y_pred + np.sum(test_X) + df_vol[postcode].iloc[random_int]\n",
    "#             arr.append(l012_next[0])\n",
    "            \n",
    "          \n",
    "#         arr_newdata = np.dot(S_u, arr)\n",
    "#         first_element = arr_newdata[0]\n",
    "#         for k in range(1, len(arr_newdata)):\n",
    "#             arr_newdata[k] = arr_newdata[k] - first_element  \n",
    "#         l_before_aj = ARIMA_modifier(arr_newdata)\n",
    "#         arr_of_arrs.append(arr)  \n",
    "        \n",
    "#     arr_of_arrs_of_arrs.append(arr_of_arrs)   \n",
    "# d3_arr = np.array(arr_of_arrs_of_arrs)\n",
    "# avg_arr = np.mean(d3_arr, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "daad8170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forecasts_sim(M, i, ARIMAModel_Dict0):\n",
    "    ARIMAModel_Dict = ARIMAModel_Dict0.copy()\n",
    "    arr_of_arrs = []\n",
    "    for steps in range (0, df_l.shape[0] - i):\n",
    "        l_before_aj = []\n",
    "        \n",
    "        for key in ARIMAModel_Dict.keys():\n",
    "            model = ARIMAModel_Dict[key]['Model']\n",
    "            l_before_aj.append(model.predict(n_periods=1, return_conf_int=False)[0])\n",
    "            \n",
    "        arr = []\n",
    "        random_int_vol = random.randint(0, 1999)\n",
    "        for postcode in list(avg_3_pivot.columns):\n",
    "            sa4 = aveHPIdf_lv3.loc[aveHPIdf_lv3['postcode'] == postcode]['sa4'].iloc[0]\n",
    "            N = df_ll.shape[0] - M\n",
    "            train_start = i - M\n",
    "            train_end = i - 1\n",
    "            test_start = i\n",
    "            train_X = df_ll.loc[train_start:train_end+1,['l0', sa4]].values\n",
    "            train_Y = df_ll.loc[train_start:train_end+1,postcode]\n",
    "            model = LinearRegression().fit(train_X, train_Y)           \n",
    "            test_X = np.array([l_before_aj[0],l_before_aj[str2key(sa4, ARIMAModel_Dict)]]).reshape(1,-1)\n",
    "            test_Y_pred = model.predict(test_X) \n",
    "            l012_next = test_Y_pred + np.sum(test_X) + df_vol[postcode].iloc[random_int_vol]\n",
    "            arr.append(l012_next[0])\n",
    "        \n",
    "        arr_newdata = np.dot(S_u, arr)\n",
    "        first_element = arr_newdata[0]\n",
    "        for k in range(1, len(arr_newdata)):\n",
    "            arr_newdata[k] = arr_newdata[k] - first_element  \n",
    "    \n",
    "        l_before_aj = []\n",
    "        for i in range(0,len(arr_newdata)):\n",
    "            key = list(df_l.columns)[i]\n",
    "            ARIMAModel_Dict[key]['Model'] = ARIMAModel_Dict[key]['Model'].update(arr_newdata[i])\n",
    "            l_new =ARIMAModel_Dict[key]['Model'].predict(n_periods=1, return_conf_int=False)[0]\n",
    "            l_before_aj.append(l_new)\n",
    "    \n",
    "\n",
    "\n",
    "        arr_of_arrs.append(arr)  \n",
    "    return arr_of_arrs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34442a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dict_strt2Arimas.pickle\", \"rb\") as file:\n",
    "    Dict_strt2Arimas = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d2fedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict_strt2Arimas = {}\n",
    "\n",
    "# for i in range(M0, df_l.shape[0]):\n",
    "#     Dict_strt2Arimas[i] = strtpoint2ARIMAModel_Dict_init(M0, i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8534922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Dict_strt2Arimas.pickle\", \"wb\") as file:\n",
    "#     pickle.dump(Dict_strt2Arimas, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b73a1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sim_forecast(i):\n",
    "    ARIMAModel_Dict0 = Dict_strt2Arimas[i]  \n",
    "    arr_of_arrs_of_arrs = []\n",
    "    for sim in range (0,Sim_Times):\n",
    "        arr_of_arrs = Forecasts_sim(M0, i, ARIMAModel_Dict0 )\n",
    "        arr_of_arrs_of_arrs.append(arr_of_arrs)\n",
    "    d3_arr = np.array(arr_of_arrs_of_arrs)\n",
    "    avg_arr = np.mean(d3_arr, axis=0)\n",
    "    arrays = [avg_arr[:, j] for j in range(avg_3_pivot.shape[1])]\n",
    "    desired_length = df_l.shape[0] - M0\n",
    "    k = 0\n",
    "    for key in Forecasts_dict.keys():\n",
    "        # Get the current length of the array (assuming it's a 1D NumPy array)\n",
    "        my_array = arrays[k]\n",
    "        current_length = len(my_array )\n",
    "        # If the current length is smaller than the desired length, pad with zeros\n",
    "        if current_length < desired_length:\n",
    "            zeros_to_add = desired_length - current_length\n",
    "            my_array = np.pad(my_array, (0, zeros_to_add), mode='constant', constant_values=0)\n",
    "        \n",
    "        Forecasts_dict[key]['window'+str(i - M0 + 1)] = my_array\n",
    "        k = k +1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8d9a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Forecasts_dict = {}\n",
    "num_cols = df_l.shape[0] - M0\n",
    "# Iterate over the columns in avg_3_pivot\n",
    "for colname in avg_3_pivot.columns:\n",
    "    new_df = pd.DataFrame({}, columns=['window'+str(k) for k in range(1, num_cols+1)])\n",
    "    Forecasts_dict[colname] = new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "113c8907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n"
     ]
    }
   ],
   "source": [
    "Sim_Times = 2\n",
    "random.seed(42)\n",
    "Sim_Times =10\n",
    "for i in range(M0, df_l.shape[0]):\n",
    "    calculate_sim_forecast(i)\n",
    "    print(i) \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "99fdd7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hier_Errors_dict = {}\n",
    "for key in Forecasts_dict.keys():\n",
    "    test_df = pd.DataFrame({})\n",
    "    data = df_dis[key].values\n",
    "    # Add a column for each window\n",
    "    for w in range(1, 18):\n",
    "        p = 18 - w\n",
    "        last_p = data[-p:]\n",
    "        new_array = np.zeros(17 - p)\n",
    "        new_array = np.concatenate((last_p, new_array))\n",
    "        test_df[f\"window{w}\"] = new_array\n",
    "    # Add the new dataframe to Hier_Errors_dict with the same key as in Forecasts_dict\n",
    "#     errors_df = Forecasts_dict[key].iloc[t]  -  test_df\n",
    "    errors_df =  -  test_df\n",
    "    Hier_Errors_dict[key] = errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "753d2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_average_hier(Dict_Name, h, M):\n",
    "    col_names = avg_3_pivot.columns\n",
    "    dfs = []\n",
    "    for col in col_names:\n",
    "        try:\n",
    "            df = Dict_Name[col]\n",
    "            dfs.append(df)\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Could not find dataframe for key {col}.\")\n",
    "\n",
    "\n",
    "    # calculate RMSE for each dataframe and find average by row\n",
    "    rmse_by_row = []\n",
    "    for df in dfs:\n",
    "        df_error = df.iloc[0: h+1, 0:len(df_l)-M-h].copy()\n",
    "        # calculate RMSE by row\n",
    "        rmse_by_row.append(np.sqrt(np.mean((df_error.values)**2)))\n",
    "\n",
    "    # calculate average of average RMSE by row\n",
    "    if rmse_by_row:\n",
    "        average_rmse = np.mean([np.mean(rmse) for rmse in rmse_by_row])\n",
    "    else:\n",
    "        average_rmse = np.nan\n",
    "\n",
    "    return average_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cc4f9110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02810783096932349"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_average_hier(Hier_Errors_dict, 1, M0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9b6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
